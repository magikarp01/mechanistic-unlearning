{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Downloading shards: 100%|██████████| 8/8 [04:59<00:00, 37.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  4.41it/s]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "MODEL_NAME = \"google/gemma-2-9b\"\n",
    "DEVICE = \"cuda\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    default_padding_side=\"left\",\n",
    "    device=DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/root/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/mechanistic-unlearning\n",
      "Forget dataset with  16  examples\n"
     ]
    }
   ],
   "source": [
    "%cd ~/mechanistic-unlearning\n",
    "from tasks.facts.CounterFactTask import CounterFactTask\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "right_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n",
    "forget_facts = 16\n",
    "\n",
    "# 'prompt' is list of string prompts\n",
    "# 'subject' is the string main subject of the prompt\n",
    "# 'first_token' is int correct answer first tokens\n",
    "# 'target_true' and 'target_false' are the string true and false answers for the prompt\n",
    "forget_kwargs = {\"forget_fact_subset\": forget_facts, \"is_forget_dataset\": True, \"train_test_split\": False}\n",
    "forget_fact_eval = CounterFactTask(batch_size=32, tokenizer=right_tokenizer, device=DEVICE, criterion=\"cross_entropy\", **forget_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The mother tongue of {} is',\n",
       " 'The mother tongue of {} is',\n",
       " '{}, developed by',\n",
       " 'The official language of {} is',\n",
       " '{} has a citizenship from',\n",
       " 'The mother tongue of {} is',\n",
       " '{}, developed by',\n",
       " '{} follows the religion of',\n",
       " '{}, a citizen of',\n",
       " 'In {}, the language spoken is',\n",
       " '{}, developed by',\n",
       " '{} is a part of the continent of',\n",
       " 'The mother tongue of {} is',\n",
       " '{}, a citizen of',\n",
       " 'The mother tongue of {} is',\n",
       " '{}, developed by']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_fact_eval.train_dataset['relation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_toks = model.tokenizer(forget_fact_eval.train_dataset['prompt'], padding=True, return_tensors=\"pt\")['input_ids']\n",
    "correct_toks = model.tokenizer(forget_fact_eval.train_dataset['target_true'], padding=True, return_tensors=\"pt\")['input_ids'][:, 1]\n",
    "wrong_toks = model.tokenizer(forget_fact_eval.train_dataset['target_false'], padding=True, return_tensors=\"pt\")['input_ids'][:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corrupt model is our clean model except with the subject embeddings set to the mean subject.\n",
    "\n",
    "When we patch our clean -> corrupt or vice versa, our corrupt nodes are nodes that have never seen the proper subject and therefore can't possibly complete the fact correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformer_lens import utils\n",
    "\n",
    "def find_subarray_occurrences(arr, subarr):\n",
    "    n = len(arr)\n",
    "    m = len(subarr)\n",
    "    occurrences = []\n",
    "\n",
    "    # Traverse through the main array\n",
    "    for i in range(n - m + 1):\n",
    "        # Check if the subarray matches starting from index i\n",
    "        if arr[i:i + m] == subarr:\n",
    "            occurrences.extend(list(range(i, i+m)))\n",
    "    \n",
    "    return occurrences\n",
    "\n",
    "\n",
    "def find_subject_occurences(prompt_toks_tensor, subject_toks_list):\n",
    "    # Find positions where convolution result matches the sum of each subarray, accounting for their actual length\n",
    "    match_positions = []\n",
    "    for i, subarray in enumerate(subject_toks_list):\n",
    "        match_positions.append(find_subarray_occurrences(prompt_toks_tensor[i].tolist(), subarray))\n",
    "\n",
    "    return match_positions\n",
    "\n",
    "all_subjects_toks = model.tokenizer.encode(\n",
    "    ''.join([' ' + x.strip() for x in forget_fact_eval.train_dataset['subject']]), \n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False\n",
    ")\n",
    "mean_subject_embedding = model.W_E[all_subjects_toks].mean(dim=1).squeeze()\n",
    "subject_toks_list = model.tokenizer(forget_fact_eval.train_dataset['subject'], add_special_tokens=False)['input_ids']\n",
    "subject_idxs = find_subject_occurences(prompt_toks, subject_toks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_logit_diff=7.992070198059082, corr_logit_diff=-0.32952821254730225\n"
     ]
    }
   ],
   "source": [
    "def corrupt_embedding_hook(act, hook):\n",
    "    if 'embed' in hook.name:\n",
    "        for batch_idx in range(act.shape[0]):\n",
    "            act[batch_idx, subject_idxs[batch_idx], :] = mean_subject_embedding\n",
    "    return act\n",
    "\n",
    "def ave_logit_diff(logits):\n",
    "    return (logits[range(logits.shape[0]), -1, correct_toks] - logits[range(logits.shape[0]), -1, wrong_toks]).mean()\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    clean_logit_diff = ave_logit_diff(model(prompt_toks.to(DEVICE))).item()\n",
    "    corr_logit_diff = ave_logit_diff(\n",
    "        model.run_with_hooks(\n",
    "            prompt_toks,\n",
    "            fwd_hooks=[\n",
    "                (utils.get_act_name(\"embed\"), corrupt_embedding_hook)\n",
    "            ]\n",
    "        )\n",
    "    ).item()\n",
    "    print(f\"{clean_logit_diff=}, {corr_logit_diff=}\")\n",
    "def noising_metric(logits):\n",
    "    logit_diff = ave_logit_diff(logits)\n",
    "    return ((logit_diff - clean_logit_diff) / (clean_logit_diff - corr_logit_diff)).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding fact extraction heads/mlps\n",
    "\n",
    "What components have the greatest direct effect to the final output?\n",
    "\n",
    "MLPs that operate on the last token position *are* counted as part of the direct path. e.g if attn head L10H9 outputs some value into the last token position and MLP20 takes this value and converts it into the right answer, L10H9 gets \"credit\" for this as part of its direct effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.path_patching.path_patching import path_patch, Node\n",
    "results = path_patch(\n",
    "    model,\n",
    "    orig_input=ioi_dataset.toks,\n",
    "    new_input=abc_dataset.toks,\n",
    "    sender_nodes=Node('z', 9, head=9), # This is the output of head 9 at layer 9\n",
    "    receiver_nodes=Node('resid_post', 11), # This is resid_post at layer 11\n",
    "    patching_metric=ioi_metric_noising,\n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding heads/MLPs that contribute the most to the heads that affect the final output\n",
    "\n",
    "Here, we look for heads/MLPs that form the representation that eventually gets moved to the last token position. One task these might be doing is \"enriching\" the subject tokens with relevant fact information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
