{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfact = load_dataset(\"azhx/counterfact\")\n",
    "wikitext = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n",
    "wikitext[\"train\"] = wikitext[\"train\"].filter(lambda row: len(row[\"text\"]) > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2-1.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2-1.5B\", default_padding_side=\"left\")\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'silu',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 11.313708498984761,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 128,\n",
       " 'd_mlp': 8960,\n",
       " 'd_model': 1536,\n",
       " 'd_vocab': 151936,\n",
       " 'd_vocab_out': 151936,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'Qwen2-1.5B',\n",
       " 'n_ctx': 2048,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': 2,\n",
       " 'n_layers': 28,\n",
       " 'n_params': 1420296192,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Qwen2ForCausalLM',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 1000000.0,\n",
       " 'rotary_dim': 128,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'Qwen/Qwen2-1.5B',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hook_embed': HookPoint(),\n",
       " 'blocks.0.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.0.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.0.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.0.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.0.attn.hook_k': HookPoint(),\n",
       " 'blocks.0.attn.hook_q': HookPoint(),\n",
       " 'blocks.0.attn.hook_v': HookPoint(),\n",
       " 'blocks.0.attn.hook_z': HookPoint(),\n",
       " 'blocks.0.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.0.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.0.attn.hook_result': HookPoint(),\n",
       " 'blocks.0.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.0.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.0.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.0.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.0.mlp.hook_post': HookPoint(),\n",
       " 'blocks.0.hook_attn_in': HookPoint(),\n",
       " 'blocks.0.hook_q_input': HookPoint(),\n",
       " 'blocks.0.hook_k_input': HookPoint(),\n",
       " 'blocks.0.hook_v_input': HookPoint(),\n",
       " 'blocks.0.hook_mlp_in': HookPoint(),\n",
       " 'blocks.0.hook_attn_out': HookPoint(),\n",
       " 'blocks.0.hook_mlp_out': HookPoint(),\n",
       " 'blocks.0.hook_resid_pre': HookPoint(),\n",
       " 'blocks.0.hook_resid_mid': HookPoint(),\n",
       " 'blocks.0.hook_resid_post': HookPoint(),\n",
       " 'blocks.1.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.1.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.1.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.1.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.1.attn.hook_k': HookPoint(),\n",
       " 'blocks.1.attn.hook_q': HookPoint(),\n",
       " 'blocks.1.attn.hook_v': HookPoint(),\n",
       " 'blocks.1.attn.hook_z': HookPoint(),\n",
       " 'blocks.1.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.1.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.1.attn.hook_result': HookPoint(),\n",
       " 'blocks.1.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.1.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.1.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.1.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.1.mlp.hook_post': HookPoint(),\n",
       " 'blocks.1.hook_attn_in': HookPoint(),\n",
       " 'blocks.1.hook_q_input': HookPoint(),\n",
       " 'blocks.1.hook_k_input': HookPoint(),\n",
       " 'blocks.1.hook_v_input': HookPoint(),\n",
       " 'blocks.1.hook_mlp_in': HookPoint(),\n",
       " 'blocks.1.hook_attn_out': HookPoint(),\n",
       " 'blocks.1.hook_mlp_out': HookPoint(),\n",
       " 'blocks.1.hook_resid_pre': HookPoint(),\n",
       " 'blocks.1.hook_resid_mid': HookPoint(),\n",
       " 'blocks.1.hook_resid_post': HookPoint(),\n",
       " 'blocks.2.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.2.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.2.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.2.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.2.attn.hook_k': HookPoint(),\n",
       " 'blocks.2.attn.hook_q': HookPoint(),\n",
       " 'blocks.2.attn.hook_v': HookPoint(),\n",
       " 'blocks.2.attn.hook_z': HookPoint(),\n",
       " 'blocks.2.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.2.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.2.attn.hook_result': HookPoint(),\n",
       " 'blocks.2.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.2.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.2.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.2.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.2.mlp.hook_post': HookPoint(),\n",
       " 'blocks.2.hook_attn_in': HookPoint(),\n",
       " 'blocks.2.hook_q_input': HookPoint(),\n",
       " 'blocks.2.hook_k_input': HookPoint(),\n",
       " 'blocks.2.hook_v_input': HookPoint(),\n",
       " 'blocks.2.hook_mlp_in': HookPoint(),\n",
       " 'blocks.2.hook_attn_out': HookPoint(),\n",
       " 'blocks.2.hook_mlp_out': HookPoint(),\n",
       " 'blocks.2.hook_resid_pre': HookPoint(),\n",
       " 'blocks.2.hook_resid_mid': HookPoint(),\n",
       " 'blocks.2.hook_resid_post': HookPoint(),\n",
       " 'blocks.3.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.3.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.3.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.3.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.3.attn.hook_k': HookPoint(),\n",
       " 'blocks.3.attn.hook_q': HookPoint(),\n",
       " 'blocks.3.attn.hook_v': HookPoint(),\n",
       " 'blocks.3.attn.hook_z': HookPoint(),\n",
       " 'blocks.3.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.3.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.3.attn.hook_result': HookPoint(),\n",
       " 'blocks.3.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.3.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.3.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.3.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.3.mlp.hook_post': HookPoint(),\n",
       " 'blocks.3.hook_attn_in': HookPoint(),\n",
       " 'blocks.3.hook_q_input': HookPoint(),\n",
       " 'blocks.3.hook_k_input': HookPoint(),\n",
       " 'blocks.3.hook_v_input': HookPoint(),\n",
       " 'blocks.3.hook_mlp_in': HookPoint(),\n",
       " 'blocks.3.hook_attn_out': HookPoint(),\n",
       " 'blocks.3.hook_mlp_out': HookPoint(),\n",
       " 'blocks.3.hook_resid_pre': HookPoint(),\n",
       " 'blocks.3.hook_resid_mid': HookPoint(),\n",
       " 'blocks.3.hook_resid_post': HookPoint(),\n",
       " 'blocks.4.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.4.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.4.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.4.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.4.attn.hook_k': HookPoint(),\n",
       " 'blocks.4.attn.hook_q': HookPoint(),\n",
       " 'blocks.4.attn.hook_v': HookPoint(),\n",
       " 'blocks.4.attn.hook_z': HookPoint(),\n",
       " 'blocks.4.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.4.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.4.attn.hook_result': HookPoint(),\n",
       " 'blocks.4.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.4.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.4.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.4.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.4.mlp.hook_post': HookPoint(),\n",
       " 'blocks.4.hook_attn_in': HookPoint(),\n",
       " 'blocks.4.hook_q_input': HookPoint(),\n",
       " 'blocks.4.hook_k_input': HookPoint(),\n",
       " 'blocks.4.hook_v_input': HookPoint(),\n",
       " 'blocks.4.hook_mlp_in': HookPoint(),\n",
       " 'blocks.4.hook_attn_out': HookPoint(),\n",
       " 'blocks.4.hook_mlp_out': HookPoint(),\n",
       " 'blocks.4.hook_resid_pre': HookPoint(),\n",
       " 'blocks.4.hook_resid_mid': HookPoint(),\n",
       " 'blocks.4.hook_resid_post': HookPoint(),\n",
       " 'blocks.5.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.5.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.5.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.5.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.5.attn.hook_k': HookPoint(),\n",
       " 'blocks.5.attn.hook_q': HookPoint(),\n",
       " 'blocks.5.attn.hook_v': HookPoint(),\n",
       " 'blocks.5.attn.hook_z': HookPoint(),\n",
       " 'blocks.5.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.5.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.5.attn.hook_result': HookPoint(),\n",
       " 'blocks.5.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.5.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.5.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.5.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.5.mlp.hook_post': HookPoint(),\n",
       " 'blocks.5.hook_attn_in': HookPoint(),\n",
       " 'blocks.5.hook_q_input': HookPoint(),\n",
       " 'blocks.5.hook_k_input': HookPoint(),\n",
       " 'blocks.5.hook_v_input': HookPoint(),\n",
       " 'blocks.5.hook_mlp_in': HookPoint(),\n",
       " 'blocks.5.hook_attn_out': HookPoint(),\n",
       " 'blocks.5.hook_mlp_out': HookPoint(),\n",
       " 'blocks.5.hook_resid_pre': HookPoint(),\n",
       " 'blocks.5.hook_resid_mid': HookPoint(),\n",
       " 'blocks.5.hook_resid_post': HookPoint(),\n",
       " 'blocks.6.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.6.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.6.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.6.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.6.attn.hook_k': HookPoint(),\n",
       " 'blocks.6.attn.hook_q': HookPoint(),\n",
       " 'blocks.6.attn.hook_v': HookPoint(),\n",
       " 'blocks.6.attn.hook_z': HookPoint(),\n",
       " 'blocks.6.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.6.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.6.attn.hook_result': HookPoint(),\n",
       " 'blocks.6.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.6.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.6.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.6.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.6.mlp.hook_post': HookPoint(),\n",
       " 'blocks.6.hook_attn_in': HookPoint(),\n",
       " 'blocks.6.hook_q_input': HookPoint(),\n",
       " 'blocks.6.hook_k_input': HookPoint(),\n",
       " 'blocks.6.hook_v_input': HookPoint(),\n",
       " 'blocks.6.hook_mlp_in': HookPoint(),\n",
       " 'blocks.6.hook_attn_out': HookPoint(),\n",
       " 'blocks.6.hook_mlp_out': HookPoint(),\n",
       " 'blocks.6.hook_resid_pre': HookPoint(),\n",
       " 'blocks.6.hook_resid_mid': HookPoint(),\n",
       " 'blocks.6.hook_resid_post': HookPoint(),\n",
       " 'blocks.7.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.7.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.7.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.7.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.7.attn.hook_k': HookPoint(),\n",
       " 'blocks.7.attn.hook_q': HookPoint(),\n",
       " 'blocks.7.attn.hook_v': HookPoint(),\n",
       " 'blocks.7.attn.hook_z': HookPoint(),\n",
       " 'blocks.7.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.7.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.7.attn.hook_result': HookPoint(),\n",
       " 'blocks.7.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.7.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.7.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.7.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.7.mlp.hook_post': HookPoint(),\n",
       " 'blocks.7.hook_attn_in': HookPoint(),\n",
       " 'blocks.7.hook_q_input': HookPoint(),\n",
       " 'blocks.7.hook_k_input': HookPoint(),\n",
       " 'blocks.7.hook_v_input': HookPoint(),\n",
       " 'blocks.7.hook_mlp_in': HookPoint(),\n",
       " 'blocks.7.hook_attn_out': HookPoint(),\n",
       " 'blocks.7.hook_mlp_out': HookPoint(),\n",
       " 'blocks.7.hook_resid_pre': HookPoint(),\n",
       " 'blocks.7.hook_resid_mid': HookPoint(),\n",
       " 'blocks.7.hook_resid_post': HookPoint(),\n",
       " 'blocks.8.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.8.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.8.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.8.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.8.attn.hook_k': HookPoint(),\n",
       " 'blocks.8.attn.hook_q': HookPoint(),\n",
       " 'blocks.8.attn.hook_v': HookPoint(),\n",
       " 'blocks.8.attn.hook_z': HookPoint(),\n",
       " 'blocks.8.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.8.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.8.attn.hook_result': HookPoint(),\n",
       " 'blocks.8.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.8.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.8.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.8.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.8.mlp.hook_post': HookPoint(),\n",
       " 'blocks.8.hook_attn_in': HookPoint(),\n",
       " 'blocks.8.hook_q_input': HookPoint(),\n",
       " 'blocks.8.hook_k_input': HookPoint(),\n",
       " 'blocks.8.hook_v_input': HookPoint(),\n",
       " 'blocks.8.hook_mlp_in': HookPoint(),\n",
       " 'blocks.8.hook_attn_out': HookPoint(),\n",
       " 'blocks.8.hook_mlp_out': HookPoint(),\n",
       " 'blocks.8.hook_resid_pre': HookPoint(),\n",
       " 'blocks.8.hook_resid_mid': HookPoint(),\n",
       " 'blocks.8.hook_resid_post': HookPoint(),\n",
       " 'blocks.9.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.9.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.9.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.9.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.9.attn.hook_k': HookPoint(),\n",
       " 'blocks.9.attn.hook_q': HookPoint(),\n",
       " 'blocks.9.attn.hook_v': HookPoint(),\n",
       " 'blocks.9.attn.hook_z': HookPoint(),\n",
       " 'blocks.9.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.9.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.9.attn.hook_result': HookPoint(),\n",
       " 'blocks.9.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.9.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.9.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.9.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.9.mlp.hook_post': HookPoint(),\n",
       " 'blocks.9.hook_attn_in': HookPoint(),\n",
       " 'blocks.9.hook_q_input': HookPoint(),\n",
       " 'blocks.9.hook_k_input': HookPoint(),\n",
       " 'blocks.9.hook_v_input': HookPoint(),\n",
       " 'blocks.9.hook_mlp_in': HookPoint(),\n",
       " 'blocks.9.hook_attn_out': HookPoint(),\n",
       " 'blocks.9.hook_mlp_out': HookPoint(),\n",
       " 'blocks.9.hook_resid_pre': HookPoint(),\n",
       " 'blocks.9.hook_resid_mid': HookPoint(),\n",
       " 'blocks.9.hook_resid_post': HookPoint(),\n",
       " 'blocks.10.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.10.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.10.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.10.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.10.attn.hook_k': HookPoint(),\n",
       " 'blocks.10.attn.hook_q': HookPoint(),\n",
       " 'blocks.10.attn.hook_v': HookPoint(),\n",
       " 'blocks.10.attn.hook_z': HookPoint(),\n",
       " 'blocks.10.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.10.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.10.attn.hook_result': HookPoint(),\n",
       " 'blocks.10.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.10.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.10.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.10.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.10.mlp.hook_post': HookPoint(),\n",
       " 'blocks.10.hook_attn_in': HookPoint(),\n",
       " 'blocks.10.hook_q_input': HookPoint(),\n",
       " 'blocks.10.hook_k_input': HookPoint(),\n",
       " 'blocks.10.hook_v_input': HookPoint(),\n",
       " 'blocks.10.hook_mlp_in': HookPoint(),\n",
       " 'blocks.10.hook_attn_out': HookPoint(),\n",
       " 'blocks.10.hook_mlp_out': HookPoint(),\n",
       " 'blocks.10.hook_resid_pre': HookPoint(),\n",
       " 'blocks.10.hook_resid_mid': HookPoint(),\n",
       " 'blocks.10.hook_resid_post': HookPoint(),\n",
       " 'blocks.11.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.11.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.11.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.11.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.11.attn.hook_k': HookPoint(),\n",
       " 'blocks.11.attn.hook_q': HookPoint(),\n",
       " 'blocks.11.attn.hook_v': HookPoint(),\n",
       " 'blocks.11.attn.hook_z': HookPoint(),\n",
       " 'blocks.11.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.11.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.11.attn.hook_result': HookPoint(),\n",
       " 'blocks.11.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.11.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.11.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.11.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.11.mlp.hook_post': HookPoint(),\n",
       " 'blocks.11.hook_attn_in': HookPoint(),\n",
       " 'blocks.11.hook_q_input': HookPoint(),\n",
       " 'blocks.11.hook_k_input': HookPoint(),\n",
       " 'blocks.11.hook_v_input': HookPoint(),\n",
       " 'blocks.11.hook_mlp_in': HookPoint(),\n",
       " 'blocks.11.hook_attn_out': HookPoint(),\n",
       " 'blocks.11.hook_mlp_out': HookPoint(),\n",
       " 'blocks.11.hook_resid_pre': HookPoint(),\n",
       " 'blocks.11.hook_resid_mid': HookPoint(),\n",
       " 'blocks.11.hook_resid_post': HookPoint(),\n",
       " 'blocks.12.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.12.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.12.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.12.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.12.attn.hook_k': HookPoint(),\n",
       " 'blocks.12.attn.hook_q': HookPoint(),\n",
       " 'blocks.12.attn.hook_v': HookPoint(),\n",
       " 'blocks.12.attn.hook_z': HookPoint(),\n",
       " 'blocks.12.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.12.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.12.attn.hook_result': HookPoint(),\n",
       " 'blocks.12.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.12.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.12.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.12.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.12.mlp.hook_post': HookPoint(),\n",
       " 'blocks.12.hook_attn_in': HookPoint(),\n",
       " 'blocks.12.hook_q_input': HookPoint(),\n",
       " 'blocks.12.hook_k_input': HookPoint(),\n",
       " 'blocks.12.hook_v_input': HookPoint(),\n",
       " 'blocks.12.hook_mlp_in': HookPoint(),\n",
       " 'blocks.12.hook_attn_out': HookPoint(),\n",
       " 'blocks.12.hook_mlp_out': HookPoint(),\n",
       " 'blocks.12.hook_resid_pre': HookPoint(),\n",
       " 'blocks.12.hook_resid_mid': HookPoint(),\n",
       " 'blocks.12.hook_resid_post': HookPoint(),\n",
       " 'blocks.13.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.13.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.13.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.13.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.13.attn.hook_k': HookPoint(),\n",
       " 'blocks.13.attn.hook_q': HookPoint(),\n",
       " 'blocks.13.attn.hook_v': HookPoint(),\n",
       " 'blocks.13.attn.hook_z': HookPoint(),\n",
       " 'blocks.13.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.13.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.13.attn.hook_result': HookPoint(),\n",
       " 'blocks.13.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.13.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.13.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.13.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.13.mlp.hook_post': HookPoint(),\n",
       " 'blocks.13.hook_attn_in': HookPoint(),\n",
       " 'blocks.13.hook_q_input': HookPoint(),\n",
       " 'blocks.13.hook_k_input': HookPoint(),\n",
       " 'blocks.13.hook_v_input': HookPoint(),\n",
       " 'blocks.13.hook_mlp_in': HookPoint(),\n",
       " 'blocks.13.hook_attn_out': HookPoint(),\n",
       " 'blocks.13.hook_mlp_out': HookPoint(),\n",
       " 'blocks.13.hook_resid_pre': HookPoint(),\n",
       " 'blocks.13.hook_resid_mid': HookPoint(),\n",
       " 'blocks.13.hook_resid_post': HookPoint(),\n",
       " 'blocks.14.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.14.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.14.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.14.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.14.attn.hook_k': HookPoint(),\n",
       " 'blocks.14.attn.hook_q': HookPoint(),\n",
       " 'blocks.14.attn.hook_v': HookPoint(),\n",
       " 'blocks.14.attn.hook_z': HookPoint(),\n",
       " 'blocks.14.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.14.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.14.attn.hook_result': HookPoint(),\n",
       " 'blocks.14.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.14.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.14.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.14.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.14.mlp.hook_post': HookPoint(),\n",
       " 'blocks.14.hook_attn_in': HookPoint(),\n",
       " 'blocks.14.hook_q_input': HookPoint(),\n",
       " 'blocks.14.hook_k_input': HookPoint(),\n",
       " 'blocks.14.hook_v_input': HookPoint(),\n",
       " 'blocks.14.hook_mlp_in': HookPoint(),\n",
       " 'blocks.14.hook_attn_out': HookPoint(),\n",
       " 'blocks.14.hook_mlp_out': HookPoint(),\n",
       " 'blocks.14.hook_resid_pre': HookPoint(),\n",
       " 'blocks.14.hook_resid_mid': HookPoint(),\n",
       " 'blocks.14.hook_resid_post': HookPoint(),\n",
       " 'blocks.15.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.15.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.15.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.15.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.15.attn.hook_k': HookPoint(),\n",
       " 'blocks.15.attn.hook_q': HookPoint(),\n",
       " 'blocks.15.attn.hook_v': HookPoint(),\n",
       " 'blocks.15.attn.hook_z': HookPoint(),\n",
       " 'blocks.15.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.15.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.15.attn.hook_result': HookPoint(),\n",
       " 'blocks.15.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.15.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.15.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.15.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.15.mlp.hook_post': HookPoint(),\n",
       " 'blocks.15.hook_attn_in': HookPoint(),\n",
       " 'blocks.15.hook_q_input': HookPoint(),\n",
       " 'blocks.15.hook_k_input': HookPoint(),\n",
       " 'blocks.15.hook_v_input': HookPoint(),\n",
       " 'blocks.15.hook_mlp_in': HookPoint(),\n",
       " 'blocks.15.hook_attn_out': HookPoint(),\n",
       " 'blocks.15.hook_mlp_out': HookPoint(),\n",
       " 'blocks.15.hook_resid_pre': HookPoint(),\n",
       " 'blocks.15.hook_resid_mid': HookPoint(),\n",
       " 'blocks.15.hook_resid_post': HookPoint(),\n",
       " 'blocks.16.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.16.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.16.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.16.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.16.attn.hook_k': HookPoint(),\n",
       " 'blocks.16.attn.hook_q': HookPoint(),\n",
       " 'blocks.16.attn.hook_v': HookPoint(),\n",
       " 'blocks.16.attn.hook_z': HookPoint(),\n",
       " 'blocks.16.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.16.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.16.attn.hook_result': HookPoint(),\n",
       " 'blocks.16.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.16.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.16.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.16.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.16.mlp.hook_post': HookPoint(),\n",
       " 'blocks.16.hook_attn_in': HookPoint(),\n",
       " 'blocks.16.hook_q_input': HookPoint(),\n",
       " 'blocks.16.hook_k_input': HookPoint(),\n",
       " 'blocks.16.hook_v_input': HookPoint(),\n",
       " 'blocks.16.hook_mlp_in': HookPoint(),\n",
       " 'blocks.16.hook_attn_out': HookPoint(),\n",
       " 'blocks.16.hook_mlp_out': HookPoint(),\n",
       " 'blocks.16.hook_resid_pre': HookPoint(),\n",
       " 'blocks.16.hook_resid_mid': HookPoint(),\n",
       " 'blocks.16.hook_resid_post': HookPoint(),\n",
       " 'blocks.17.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.17.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.17.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.17.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.17.attn.hook_k': HookPoint(),\n",
       " 'blocks.17.attn.hook_q': HookPoint(),\n",
       " 'blocks.17.attn.hook_v': HookPoint(),\n",
       " 'blocks.17.attn.hook_z': HookPoint(),\n",
       " 'blocks.17.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.17.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.17.attn.hook_result': HookPoint(),\n",
       " 'blocks.17.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.17.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.17.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.17.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.17.mlp.hook_post': HookPoint(),\n",
       " 'blocks.17.hook_attn_in': HookPoint(),\n",
       " 'blocks.17.hook_q_input': HookPoint(),\n",
       " 'blocks.17.hook_k_input': HookPoint(),\n",
       " 'blocks.17.hook_v_input': HookPoint(),\n",
       " 'blocks.17.hook_mlp_in': HookPoint(),\n",
       " 'blocks.17.hook_attn_out': HookPoint(),\n",
       " 'blocks.17.hook_mlp_out': HookPoint(),\n",
       " 'blocks.17.hook_resid_pre': HookPoint(),\n",
       " 'blocks.17.hook_resid_mid': HookPoint(),\n",
       " 'blocks.17.hook_resid_post': HookPoint(),\n",
       " 'blocks.18.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.18.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.18.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.18.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.18.attn.hook_k': HookPoint(),\n",
       " 'blocks.18.attn.hook_q': HookPoint(),\n",
       " 'blocks.18.attn.hook_v': HookPoint(),\n",
       " 'blocks.18.attn.hook_z': HookPoint(),\n",
       " 'blocks.18.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.18.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.18.attn.hook_result': HookPoint(),\n",
       " 'blocks.18.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.18.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.18.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.18.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.18.mlp.hook_post': HookPoint(),\n",
       " 'blocks.18.hook_attn_in': HookPoint(),\n",
       " 'blocks.18.hook_q_input': HookPoint(),\n",
       " 'blocks.18.hook_k_input': HookPoint(),\n",
       " 'blocks.18.hook_v_input': HookPoint(),\n",
       " 'blocks.18.hook_mlp_in': HookPoint(),\n",
       " 'blocks.18.hook_attn_out': HookPoint(),\n",
       " 'blocks.18.hook_mlp_out': HookPoint(),\n",
       " 'blocks.18.hook_resid_pre': HookPoint(),\n",
       " 'blocks.18.hook_resid_mid': HookPoint(),\n",
       " 'blocks.18.hook_resid_post': HookPoint(),\n",
       " 'blocks.19.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.19.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.19.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.19.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.19.attn.hook_k': HookPoint(),\n",
       " 'blocks.19.attn.hook_q': HookPoint(),\n",
       " 'blocks.19.attn.hook_v': HookPoint(),\n",
       " 'blocks.19.attn.hook_z': HookPoint(),\n",
       " 'blocks.19.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.19.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.19.attn.hook_result': HookPoint(),\n",
       " 'blocks.19.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.19.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.19.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.19.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.19.mlp.hook_post': HookPoint(),\n",
       " 'blocks.19.hook_attn_in': HookPoint(),\n",
       " 'blocks.19.hook_q_input': HookPoint(),\n",
       " 'blocks.19.hook_k_input': HookPoint(),\n",
       " 'blocks.19.hook_v_input': HookPoint(),\n",
       " 'blocks.19.hook_mlp_in': HookPoint(),\n",
       " 'blocks.19.hook_attn_out': HookPoint(),\n",
       " 'blocks.19.hook_mlp_out': HookPoint(),\n",
       " 'blocks.19.hook_resid_pre': HookPoint(),\n",
       " 'blocks.19.hook_resid_mid': HookPoint(),\n",
       " 'blocks.19.hook_resid_post': HookPoint(),\n",
       " 'blocks.20.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.20.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.20.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.20.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.20.attn.hook_k': HookPoint(),\n",
       " 'blocks.20.attn.hook_q': HookPoint(),\n",
       " 'blocks.20.attn.hook_v': HookPoint(),\n",
       " 'blocks.20.attn.hook_z': HookPoint(),\n",
       " 'blocks.20.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.20.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.20.attn.hook_result': HookPoint(),\n",
       " 'blocks.20.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.20.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.20.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.20.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.20.mlp.hook_post': HookPoint(),\n",
       " 'blocks.20.hook_attn_in': HookPoint(),\n",
       " 'blocks.20.hook_q_input': HookPoint(),\n",
       " 'blocks.20.hook_k_input': HookPoint(),\n",
       " 'blocks.20.hook_v_input': HookPoint(),\n",
       " 'blocks.20.hook_mlp_in': HookPoint(),\n",
       " 'blocks.20.hook_attn_out': HookPoint(),\n",
       " 'blocks.20.hook_mlp_out': HookPoint(),\n",
       " 'blocks.20.hook_resid_pre': HookPoint(),\n",
       " 'blocks.20.hook_resid_mid': HookPoint(),\n",
       " 'blocks.20.hook_resid_post': HookPoint(),\n",
       " 'blocks.21.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.21.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.21.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.21.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.21.attn.hook_k': HookPoint(),\n",
       " 'blocks.21.attn.hook_q': HookPoint(),\n",
       " 'blocks.21.attn.hook_v': HookPoint(),\n",
       " 'blocks.21.attn.hook_z': HookPoint(),\n",
       " 'blocks.21.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.21.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.21.attn.hook_result': HookPoint(),\n",
       " 'blocks.21.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.21.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.21.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.21.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.21.mlp.hook_post': HookPoint(),\n",
       " 'blocks.21.hook_attn_in': HookPoint(),\n",
       " 'blocks.21.hook_q_input': HookPoint(),\n",
       " 'blocks.21.hook_k_input': HookPoint(),\n",
       " 'blocks.21.hook_v_input': HookPoint(),\n",
       " 'blocks.21.hook_mlp_in': HookPoint(),\n",
       " 'blocks.21.hook_attn_out': HookPoint(),\n",
       " 'blocks.21.hook_mlp_out': HookPoint(),\n",
       " 'blocks.21.hook_resid_pre': HookPoint(),\n",
       " 'blocks.21.hook_resid_mid': HookPoint(),\n",
       " 'blocks.21.hook_resid_post': HookPoint(),\n",
       " 'blocks.22.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.22.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.22.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.22.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.22.attn.hook_k': HookPoint(),\n",
       " 'blocks.22.attn.hook_q': HookPoint(),\n",
       " 'blocks.22.attn.hook_v': HookPoint(),\n",
       " 'blocks.22.attn.hook_z': HookPoint(),\n",
       " 'blocks.22.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.22.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.22.attn.hook_result': HookPoint(),\n",
       " 'blocks.22.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.22.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.22.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.22.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.22.mlp.hook_post': HookPoint(),\n",
       " 'blocks.22.hook_attn_in': HookPoint(),\n",
       " 'blocks.22.hook_q_input': HookPoint(),\n",
       " 'blocks.22.hook_k_input': HookPoint(),\n",
       " 'blocks.22.hook_v_input': HookPoint(),\n",
       " 'blocks.22.hook_mlp_in': HookPoint(),\n",
       " 'blocks.22.hook_attn_out': HookPoint(),\n",
       " 'blocks.22.hook_mlp_out': HookPoint(),\n",
       " 'blocks.22.hook_resid_pre': HookPoint(),\n",
       " 'blocks.22.hook_resid_mid': HookPoint(),\n",
       " 'blocks.22.hook_resid_post': HookPoint(),\n",
       " 'blocks.23.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.23.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.23.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.23.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.23.attn.hook_k': HookPoint(),\n",
       " 'blocks.23.attn.hook_q': HookPoint(),\n",
       " 'blocks.23.attn.hook_v': HookPoint(),\n",
       " 'blocks.23.attn.hook_z': HookPoint(),\n",
       " 'blocks.23.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.23.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.23.attn.hook_result': HookPoint(),\n",
       " 'blocks.23.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.23.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.23.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.23.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.23.mlp.hook_post': HookPoint(),\n",
       " 'blocks.23.hook_attn_in': HookPoint(),\n",
       " 'blocks.23.hook_q_input': HookPoint(),\n",
       " 'blocks.23.hook_k_input': HookPoint(),\n",
       " 'blocks.23.hook_v_input': HookPoint(),\n",
       " 'blocks.23.hook_mlp_in': HookPoint(),\n",
       " 'blocks.23.hook_attn_out': HookPoint(),\n",
       " 'blocks.23.hook_mlp_out': HookPoint(),\n",
       " 'blocks.23.hook_resid_pre': HookPoint(),\n",
       " 'blocks.23.hook_resid_mid': HookPoint(),\n",
       " 'blocks.23.hook_resid_post': HookPoint(),\n",
       " 'blocks.24.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.24.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.24.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.24.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.24.attn.hook_k': HookPoint(),\n",
       " 'blocks.24.attn.hook_q': HookPoint(),\n",
       " 'blocks.24.attn.hook_v': HookPoint(),\n",
       " 'blocks.24.attn.hook_z': HookPoint(),\n",
       " 'blocks.24.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.24.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.24.attn.hook_result': HookPoint(),\n",
       " 'blocks.24.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.24.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.24.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.24.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.24.mlp.hook_post': HookPoint(),\n",
       " 'blocks.24.hook_attn_in': HookPoint(),\n",
       " 'blocks.24.hook_q_input': HookPoint(),\n",
       " 'blocks.24.hook_k_input': HookPoint(),\n",
       " 'blocks.24.hook_v_input': HookPoint(),\n",
       " 'blocks.24.hook_mlp_in': HookPoint(),\n",
       " 'blocks.24.hook_attn_out': HookPoint(),\n",
       " 'blocks.24.hook_mlp_out': HookPoint(),\n",
       " 'blocks.24.hook_resid_pre': HookPoint(),\n",
       " 'blocks.24.hook_resid_mid': HookPoint(),\n",
       " 'blocks.24.hook_resid_post': HookPoint(),\n",
       " 'blocks.25.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.25.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.25.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.25.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.25.attn.hook_k': HookPoint(),\n",
       " 'blocks.25.attn.hook_q': HookPoint(),\n",
       " 'blocks.25.attn.hook_v': HookPoint(),\n",
       " 'blocks.25.attn.hook_z': HookPoint(),\n",
       " 'blocks.25.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.25.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.25.attn.hook_result': HookPoint(),\n",
       " 'blocks.25.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.25.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.25.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.25.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.25.mlp.hook_post': HookPoint(),\n",
       " 'blocks.25.hook_attn_in': HookPoint(),\n",
       " 'blocks.25.hook_q_input': HookPoint(),\n",
       " 'blocks.25.hook_k_input': HookPoint(),\n",
       " 'blocks.25.hook_v_input': HookPoint(),\n",
       " 'blocks.25.hook_mlp_in': HookPoint(),\n",
       " 'blocks.25.hook_attn_out': HookPoint(),\n",
       " 'blocks.25.hook_mlp_out': HookPoint(),\n",
       " 'blocks.25.hook_resid_pre': HookPoint(),\n",
       " 'blocks.25.hook_resid_mid': HookPoint(),\n",
       " 'blocks.25.hook_resid_post': HookPoint(),\n",
       " 'blocks.26.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.26.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.26.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.26.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.26.attn.hook_k': HookPoint(),\n",
       " 'blocks.26.attn.hook_q': HookPoint(),\n",
       " 'blocks.26.attn.hook_v': HookPoint(),\n",
       " 'blocks.26.attn.hook_z': HookPoint(),\n",
       " 'blocks.26.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.26.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.26.attn.hook_result': HookPoint(),\n",
       " 'blocks.26.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.26.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.26.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.26.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.26.mlp.hook_post': HookPoint(),\n",
       " 'blocks.26.hook_attn_in': HookPoint(),\n",
       " 'blocks.26.hook_q_input': HookPoint(),\n",
       " 'blocks.26.hook_k_input': HookPoint(),\n",
       " 'blocks.26.hook_v_input': HookPoint(),\n",
       " 'blocks.26.hook_mlp_in': HookPoint(),\n",
       " 'blocks.26.hook_attn_out': HookPoint(),\n",
       " 'blocks.26.hook_mlp_out': HookPoint(),\n",
       " 'blocks.26.hook_resid_pre': HookPoint(),\n",
       " 'blocks.26.hook_resid_mid': HookPoint(),\n",
       " 'blocks.26.hook_resid_post': HookPoint(),\n",
       " 'blocks.27.ln1.hook_scale': HookPoint(),\n",
       " 'blocks.27.ln1.hook_normalized': HookPoint(),\n",
       " 'blocks.27.ln2.hook_scale': HookPoint(),\n",
       " 'blocks.27.ln2.hook_normalized': HookPoint(),\n",
       " 'blocks.27.attn.hook_k': HookPoint(),\n",
       " 'blocks.27.attn.hook_q': HookPoint(),\n",
       " 'blocks.27.attn.hook_v': HookPoint(),\n",
       " 'blocks.27.attn.hook_z': HookPoint(),\n",
       " 'blocks.27.attn.hook_attn_scores': HookPoint(),\n",
       " 'blocks.27.attn.hook_pattern': HookPoint(),\n",
       " 'blocks.27.attn.hook_result': HookPoint(),\n",
       " 'blocks.27.attn.hook_rot_k': HookPoint(),\n",
       " 'blocks.27.attn.hook_rot_q': HookPoint(),\n",
       " 'blocks.27.mlp.hook_pre': HookPoint(),\n",
       " 'blocks.27.mlp.hook_pre_linear': HookPoint(),\n",
       " 'blocks.27.mlp.hook_post': HookPoint(),\n",
       " 'blocks.27.hook_attn_in': HookPoint(),\n",
       " 'blocks.27.hook_q_input': HookPoint(),\n",
       " 'blocks.27.hook_k_input': HookPoint(),\n",
       " 'blocks.27.hook_v_input': HookPoint(),\n",
       " 'blocks.27.hook_mlp_in': HookPoint(),\n",
       " 'blocks.27.hook_attn_out': HookPoint(),\n",
       " 'blocks.27.hook_mlp_out': HookPoint(),\n",
       " 'blocks.27.hook_resid_pre': HookPoint(),\n",
       " 'blocks.27.hook_resid_mid': HookPoint(),\n",
       " 'blocks.27.hook_resid_post': HookPoint(),\n",
       " 'ln_final.hook_scale': HookPoint(),\n",
       " 'ln_final.hook_normalized': HookPoint()}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hook_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(model.tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from transformer_lens.HookedTransformer import HookPoint\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EditRequest:\n",
    "    prompt: str\n",
    "    subject: str\n",
    "    target_true: str\n",
    "    target_new: str\n",
    "\n",
    "# TODO: Make these layer specific functions:\n",
    "\n",
    "def get_key_cov(model: HookedTransformer, wiki_dataset):\n",
    "    ''' Estimate E[KK^T] using 100k samples from wiki_dataset. '''\n",
    "    K = torch.zeros(model.cfg.d_mlp, device=device)\n",
    "\n",
    "    def collect_ks_hook(act: Float[Tensor, \"batch seq d_mlp\"], hook: HookPoint):\n",
    "        nonlocal K\n",
    "        K += einops.reduce(act, \"batch seq d_mlp -> d_mlp\", \"sum\")\n",
    "        return act\n",
    "\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for text in wiki_dataset[\"train\"][\"text\"]:\n",
    "            if num_samples >= 100_000:\n",
    "                break\n",
    "            toks = model.tokenizer(text, return_tensors=\"pt\", max_length=10_000, truncation=True, padding=False)[\"input_ids\"]\n",
    "            num_samples += toks.shape[1]\n",
    "            model.run_with_hooks(\n",
    "                toks,\n",
    "                fwd_hooks = [(lambda name: \"mlp.hook_post\" in name, collect_ks_hook)]\n",
    "            )\n",
    "    K /= num_samples\n",
    "    return einops.einsum(K, K, \"i, j -> i j\")\n",
    "\n",
    "def correctness_filter(model, dataset, verbose=False):\n",
    "    ''' Filter out any examples that the model gets wrong. '''\n",
    "    def get_correctness(model, examples):\n",
    "        ''' Populate dataset with first token of correct answer and model answer '''\n",
    "        true_string = [' ' + r['target_true']['str'] for r in examples['requested_rewrite']]\n",
    "        edit_string = [' ' + r['target_new']['str'] for r in examples['requested_rewrite']]\n",
    "        question_string = [r['prompt'].format(r['subject']) for r in examples['requested_rewrite']]\n",
    "\n",
    "        orig_padding_side = model.tokenizer.padding_side\n",
    "        model.tokenizer.padding_side = \"right\"\n",
    "        correct_tokens = list(model.tokenizer(true_string, return_tensors=\"pt\", padding=True)[\"input_ids\"][:, 0])\n",
    "        edit_tokens = model.tokenizer(edit_string, return_tensors=\"pt\", padding=True)[\"input_ids\"][:, 0]\n",
    "        model.tokenizer.padding_side = orig_padding_side\n",
    "\n",
    "        question_tokens = model.tokenizer(question_string, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "        scaled_logits = torch.nn.functional.softmax(model(question_tokens)[:, -1, :], dim=-1)\n",
    "        is_model_correct = list(scaled_logits[range(len(scaled_logits)), correct_tokens] > scaled_logits[range(len(scaled_logits)), edit_tokens])\n",
    "        if verbose:\n",
    "            print(f\"String: {true_string[0]} tokenized as {correct_tokens[0]}, model is correct? {is_model_correct[0]}\")\n",
    "\n",
    "        return {\"correct_token\": correct_tokens, \"is_model_correct\": is_model_correct}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dataset = dataset.map(\n",
    "            lambda row: get_correctness(model, row),\n",
    "            batched=True,\n",
    "            batch_size=1_000\n",
    "        )\n",
    "        dataset = dataset.filter(lambda x: x[\"is_model_correct\"])\n",
    "\n",
    "    assert all(cfact[\"train\"][\"is_model_correct\"]), \"Filter failed, model is not correct on all examples\"\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_subject_representations(model, edit_request: EditRequest):\n",
    "    ''' Get the subject representations k* for the fact. '''\n",
    "\n",
    "    K = torch.zeros(model.cfg.d_mlp, device=device)\n",
    "\n",
    "    def collect_ks_hook(act: Float[Tensor, \"batch seq d_mlp\"], hook: HookPoint):\n",
    "        nonlocal K\n",
    "        K += einops.reduce(act[:, -1, :], \"batch d_mlp -> d_mlp\", \"sum\")\n",
    "        return act\n",
    "\n",
    "    start_input = torch.zeros((10, 1), device=device, dtype=torch.int)\n",
    "    start_input[:, 0] = model.tokenizer.bos_token_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # According to E.5 they use 10 prefixes of length 5 and 10 prefixes of length 10\n",
    "        prefix_5 = model.tokenizer.batch_decode(model.generate(start_input, max_new_tokens=5))\n",
    "        prefix_10 = model.tokenizer.batch_decode(model.generate(start_input, max_new_tokens=10))\n",
    "        subject_prompts = [p + f\". {edit_request.subject}\" for p in prefix_5 + prefix_10]\n",
    "        subject_toks = model.tokenizer(subject_prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)[\"input_ids\"]\n",
    "        model.run_with_hooks(\n",
    "            subject_toks,\n",
    "            fwd_hooks = [(lambda name: \"mlp.hook_post\" in name, collect_ks_hook)]\n",
    "        )\n",
    "    return K / len(subject_prompts)\n",
    "\n",
    "\n",
    "def get_value_representations(model, edit_request):\n",
    "    ''' Get the value representation v* for a fact in the dataset. '''\n",
    "    model = model.eval()  # Disable gradients on the model, we are optimizing a new vector here\n",
    "\n",
    "    start_input = torch.zeros((10, 1), device=device, dtype=torch.int)\n",
    "    start_input[:, 0] = model.tokenizer.bos_token_id\n",
    "\n",
    "    prefix_5 = model.tokenizer.batch_decode(model.generate(start_input, max_new_tokens=5))\n",
    "    prefix_10 = model.tokenizer.batch_decode(model.generate(start_input, max_new_tokens=10))\n",
    "\n",
    "\n",
    "\n",
    "def get_rome_edit(model, edit_request, cov_dataset):\n",
    "    ''' Implement the ROME edit function. \n",
    "        W_hat = W + A(C^-1 * k_star)^T, where:\n",
    "        k_star = E[mlp_out(x_i + subject)], x_i is random prefix\n",
    "        C = covariance matrix of keys\n",
    "        A = (v_star - W * k_star) / (C^-1 * k_star)^T * k_star\n",
    "        v_star optimizes log probability of outputting correct object and KL with original model\n",
    "    '''\n",
    "    k_star = get_subject_representations(model, edit_request)\n",
    "    v_star = get_value_representations(model, edit_request)\n",
    "    C = get_key_cov(model, cov_dataset)\n",
    "    A = (v_star - model.W * k_star) / torch.matmul(torch.linalg.inv(C), k_star).T * k_star\n",
    "\n",
    "    return torch.matmul(A, torch.linalg.inv(C).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41508, 20668]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.encode(\" Isaac Newton\", add_special_tokens=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = get_key_cov(model, wikitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:00<00:00,  9.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>\\n',\n",
       " '<|endoftext|>import boto3\\nimport',\n",
       " '<|endoftext|>Imperialist Adm',\n",
       " '<|endoftext|>F Commands with MATLAB R',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|><?php\\n// This',\n",
       " '<|endoftext|>Human: There is a',\n",
       " '<|endoftext|>Before this question has a',\n",
       " '<|endoftext|> ',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_input = torch.zeros((10, 1), device=device, dtype=torch.int)\n",
    "start_input[:, 0] = model.tokenizer.bos_token_id\n",
    "model.tokenizer.batch_decode(model.generate(start_input, max_new_tokens=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   5%|         | 1000/19728 [00:05<01:43, 180.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  French tokenized as 8585, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  10%|         | 2000/19728 [00:11<01:41, 174.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Finnish tokenized as 57853, model is correct? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  15%|        | 3000/19728 [00:15<01:26, 193.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  biology tokenized as 33358, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  20%|        | 4000/19728 [00:21<01:21, 194.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Moscow tokenized as 22415, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  25%|       | 5000/19728 [00:26<01:18, 187.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  French tokenized as 8585, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  30%|       | 6000/19728 [00:31<01:11, 193.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  bishop tokenized as 53206, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  35%|      | 7000/19728 [00:36<01:02, 202.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Europe tokenized as 4505, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  41%|      | 8000/19728 [00:41<00:58, 199.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Nokia tokenized as 35706, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  46%|     | 9000/19728 [00:46<00:54, 195.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  goaltender tokenized as 79622, model is correct? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  51%|     | 10000/19728 [00:51<00:49, 198.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Microsoft tokenized as 5100, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  56%|    | 11000/19728 [00:57<00:45, 191.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Philadelphia tokenized as 19335, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  61%|    | 12000/19728 [01:01<00:39, 195.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Germany tokenized as 9856, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  66%|   | 13000/19728 [01:08<00:38, 176.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Spanish tokenized as 15154, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  71%|   | 14000/19728 [01:14<00:33, 173.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Kerala tokenized as 60407, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  76%|  | 15000/19728 [01:19<00:26, 180.92 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Oslo tokenized as 57858, model is correct? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  81%|  | 16000/19728 [01:25<00:21, 174.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Sweden tokenized as 23190, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  86%| | 17000/19728 [01:30<00:14, 185.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Paris tokenized as 12095, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  91%| | 18000/19728 [01:35<00:09, 187.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Amsterdam tokenized as 37741, model is correct? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  96%|| 19000/19728 [01:41<00:03, 188.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  actor tokenized as 12089, model is correct? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 19728/19728 [01:45<00:00, 187.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Antarctica tokenized as 71687, model is correct? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|| 19728/19728 [00:02<00:00, 9113.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "cfact[\"train\"] = correctness_filter(model, cfact[\"train\"], verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
