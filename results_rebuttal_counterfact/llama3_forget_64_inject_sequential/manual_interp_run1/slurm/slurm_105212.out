/data/phillip_guo/mechanistic-unlearning
Loading args from config file: results_rebuttal_counterfact/llama3_forget_64_inject_sequential/manual_interp_run1/config.json
==========ARGS==========
Namespace(config_path='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/manual_interp_run1/config.json', save_dir='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/manual_interp_run1', model_type='llama-3-8b', forget_split='first_64_partitioned_unsplit', inject_fact=True, localization_type='manual_interp', run_id=1, combine_heads=True, train_batch_size=4, eval_batch_size=32, learning_rate=5e-05, grad_accum_steps=16, mixed_precision=False, n_epochs=25, beta=3, clip_grad=1, evaluate_every=5, n_eval_iters=5, deep_evaluate_every=None, do_adversarial_evals=True, n_mc_shots=8, do_side_effects_evals=True, check_all_logits=False, use_wandb=True, save_model=False, push_to_hub=False, do_full_mmlu_evals=True, do_relearning_evals=True, n_relearn_iters=20, n_relearn_facts=32, lora_rank=512, target_modules='all-linear', relearning_lr=0.0002, forget_loss_coef=0.2, do_softprompt_evals=True, softprompt_attack_batch_size=16, num_softprompts=4)
==========END ARGS==========
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.44s/it]
wandb: Currently logged in as: philliphguo (quirky_lats_at_mats). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /data/phillip_guo/mechanistic-unlearning/wandb/run-20241126_082028-dm7ghkpm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuning_counterfact_manual_interp_forget_split='first_64_partitioned_unsplit'_inject_fact=True_run_id=1
wandb: ⭐️ View project at https://wandb.ai/quirky_lats_at_mats/circuit_breaking
wandb: 🚀 View run at https://wandb.ai/quirky_lats_at_mats/circuit_breaking/runs/dm7ghkpm
Memory at start for manual_interp: 0.0
==========Partition 0, 0_16==========
Manual param count for partition 0:  704643072
Current partition forget split: first_64_partition_0_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
0       0  ...         0
1      22  ...        22
2      29  ...        29
3      48  ...        48
4      54  ...        54
5      60  ...        60
6      86  ...        86
7     104  ...       104
8     109  ...       109
9     116  ...       116
10    119  ...       119
11    125  ...       125
12    137  ...       137
13    139  ...       139
14    168  ...       168
15    181  ...       181

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
Number of parameters in manual_interp localization: 704643072
final_components=['blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_gate', 'blocks.5.mlp.hook_post', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_gate', 'blocks.6.mlp.hook_post', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_gate', 'blocks.9.mlp.hook_post', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_gate', 'blocks.13.mlp.hook_post']
  0%|          | 0/25 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  4%|▍         | 1/25 [00:26<10:37, 26.56s/it]  8%|▊         | 2/25 [00:32<05:26, 14.22s/it] 12%|█▏        | 3/25 [00:37<03:46, 10.30s/it] 16%|█▌        | 4/25 [00:43<03:00,  8.57s/it] 20%|██        | 5/25 [00:49<02:33,  7.66s/it] 24%|██▍       | 6/25 [01:00<02:43,  8.61s/it] 28%|██▊       | 7/25 [01:06<02:19,  7.73s/it] 32%|███▏      | 8/25 [01:12<02:04,  7.32s/it] 36%|███▌      | 9/25 [01:18<01:49,  6.85s/it] 40%|████      | 10/25 [01:24<01:38,  6.59s/it] 44%|████▍     | 11/25 [01:34<01:49,  7.79s/it] 48%|████▊     | 12/25 [01:40<01:33,  7.19s/it] 52%|█████▏    | 13/25 [01:46<01:20,  6.75s/it] 56%|█████▌    | 14/25 [01:52<01:11,  6.51s/it] 60%|██████    | 15/25 [01:58<01:02,  6.29s/it] 64%|██████▍   | 16/25 [02:08<01:08,  7.56s/it] 68%|██████▊   | 17/25 [02:14<00:57,  7.14s/it] 72%|███████▏  | 18/25 [02:20<00:47,  6.78s/it] 76%|███████▌  | 19/25 [02:26<00:38,  6.48s/it] 80%|████████  | 20/25 [02:32<00:31,  6.24s/it] 84%|████████▍ | 21/25 [02:43<00:30,  7.63s/it] 88%|████████▊ | 22/25 [02:49<00:21,  7.12s/it] 92%|█████████▏| 23/25 [02:54<00:13,  6.76s/it] 96%|█████████▌| 24/25 [03:00<00:06,  6.42s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [04:20<00:00, 28.55s/it]100%|██████████| 25/25 [04:20<00:00, 10.43s/it]
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  17.59841251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 1, 16_32==========
Manual param count for partition 1:  1056964608
Current partition forget split: first_64_partition_1_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
16    196  ...       196
17    200  ...       200
18    215  ...       215
19    225  ...       225
20    231  ...       231
21    234  ...       234
22    239  ...       239
23    241  ...       241
24    248  ...       248
25    253  ...       253
26    272  ...       272
27    301  ...       301
28    307  ...       307
29    333  ...       333
30    335  ...       335
31    339  ...       339

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
Number of parameters in manual_interp localization: 1056964608
final_components=['blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_gate', 'blocks.2.mlp.hook_post', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_gate', 'blocks.5.mlp.hook_post', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_gate', 'blocks.6.mlp.hook_post', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_gate', 'blocks.7.mlp.hook_post', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_gate', 'blocks.9.mlp.hook_post', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_gate', 'blocks.10.mlp.hook_post']
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:23<09:34, 23.95s/it]  8%|▊         | 2/25 [00:30<05:09, 13.44s/it] 12%|█▏        | 3/25 [00:36<03:42, 10.09s/it] 16%|█▌        | 4/25 [00:42<03:02,  8.67s/it] 20%|██        | 5/25 [00:48<02:35,  7.75s/it] 24%|██▍       | 6/25 [00:59<02:49,  8.92s/it] 28%|██▊       | 7/25 [01:06<02:25,  8.08s/it] 32%|███▏      | 8/25 [01:12<02:08,  7.56s/it] 36%|███▌      | 9/25 [01:19<01:54,  7.17s/it] 40%|████      | 10/25 [01:25<01:44,  6.94s/it] 44%|████▍     | 11/25 [01:36<01:53,  8.11s/it] 48%|████▊     | 12/25 [01:42<01:39,  7.67s/it] 52%|█████▏    | 13/25 [01:49<01:27,  7.27s/it] 56%|█████▌    | 14/25 [01:55<01:17,  7.05s/it] 60%|██████    | 15/25 [02:02<01:08,  6.84s/it] 64%|██████▍   | 16/25 [02:13<01:12,  8.05s/it] 68%|██████▊   | 17/25 [02:19<01:00,  7.54s/it] 72%|███████▏  | 18/25 [02:25<00:50,  7.22s/it] 76%|███████▌  | 19/25 [02:32<00:41,  6.96s/it] 80%|████████  | 20/25 [02:38<00:33,  6.77s/it] 84%|████████▍ | 21/25 [02:49<00:32,  8.17s/it] 88%|████████▊ | 22/25 [02:56<00:22,  7.65s/it] 92%|█████████▏| 23/25 [03:02<00:14,  7.21s/it] 96%|█████████▌| 24/25 [03:08<00:06,  6.95s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [04:29<00:00, 29.09s/it]100%|██████████| 25/25 [04:29<00:00, 10.79s/it]
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  18.91091251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 2, 32_48==========
Manual param count for partition 2:  880803840
Current partition forget split: first_64_partition_2_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
32    354  ...       354
33    357  ...       357
34    361  ...       361
35    371  ...       371
36    372  ...       372
37    380  ...       380
38    381  ...       381
39    391  ...       391
40    397  ...       397
41    421  ...       421
42    422  ...       422
43    424  ...       424
44    428  ...       428
45    433  ...       433
46    461  ...       461
47    475  ...       475

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
Number of parameters in manual_interp localization: 880803840
final_components=['blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_gate', 'blocks.2.mlp.hook_post', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_gate', 'blocks.4.mlp.hook_post', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_gate', 'blocks.5.mlp.hook_post', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_gate', 'blocks.6.mlp.hook_post', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_gate', 'blocks.7.mlp.hook_post']
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:23<09:23, 23.46s/it]  8%|▊         | 2/25 [00:29<05:05, 13.30s/it] 12%|█▏        | 3/25 [00:35<03:39,  9.97s/it] 16%|█▌        | 4/25 [00:41<02:57,  8.47s/it] 20%|██        | 5/25 [00:47<02:32,  7.61s/it] 24%|██▍       | 6/25 [00:59<02:47,  8.83s/it] 28%|██▊       | 7/25 [01:05<02:26,  8.16s/it] 32%|███▏      | 8/25 [01:12<02:08,  7.53s/it] 36%|███▌      | 9/25 [01:18<01:55,  7.20s/it] 40%|████      | 10/25 [01:25<01:45,  7.02s/it] 44%|████▍     | 11/25 [01:36<01:55,  8.24s/it] 48%|████▊     | 12/25 [01:42<01:39,  7.64s/it] 52%|█████▏    | 13/25 [01:49<01:27,  7.31s/it] 56%|█████▌    | 14/25 [01:55<01:18,  7.10s/it] 60%|██████    | 15/25 [02:02<01:08,  6.89s/it] 64%|██████▍   | 16/25 [02:13<01:13,  8.20s/it] 68%|██████▊   | 17/25 [02:19<01:01,  7.67s/it] 72%|███████▏  | 18/25 [02:26<00:51,  7.33s/it] 76%|███████▌  | 19/25 [02:32<00:42,  7.03s/it] 80%|████████  | 20/25 [02:38<00:33,  6.78s/it] 84%|████████▍ | 21/25 [02:49<00:31,  7.97s/it] 88%|████████▊ | 22/25 [02:56<00:22,  7.58s/it] 92%|█████████▏| 23/25 [03:02<00:14,  7.31s/it] 96%|█████████▌| 24/25 [03:09<00:07,  7.01s/it]