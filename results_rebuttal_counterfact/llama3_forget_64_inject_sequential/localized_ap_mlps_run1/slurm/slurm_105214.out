/data/phillip_guo/mechanistic-unlearning
Loading args from config file: results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ap_mlps_run1/config.json
==========ARGS==========
Namespace(config_path='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ap_mlps_run1/config.json', save_dir='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ap_mlps_run1', model_type='llama-3-8b', forget_split='first_64_partitioned_unsplit', inject_fact=True, localization_type='localized_ap_mlps', run_id=1, combine_heads=True, train_batch_size=4, eval_batch_size=32, learning_rate=5e-05, grad_accum_steps=16, mixed_precision=False, n_epochs=25, beta=3, clip_grad=1, evaluate_every=5, n_eval_iters=5, deep_evaluate_every=None, do_adversarial_evals=True, n_mc_shots=8, do_side_effects_evals=True, check_all_logits=False, use_wandb=True, save_model=False, push_to_hub=False, do_full_mmlu_evals=True, do_relearning_evals=True, n_relearn_iters=20, n_relearn_facts=32, lora_rank=512, target_modules='all-linear', relearning_lr=0.0002, forget_loss_coef=0.2, do_softprompt_evals=True, softprompt_attack_batch_size=16, num_softprompts=4)
==========END ARGS==========
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.44s/it]
wandb: Currently logged in as: philliphguo (quirky_lats_at_mats). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /data/phillip_guo/mechanistic-unlearning/wandb/run-20241126_082028-nsqh90gf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuning_counterfact_localized_ap_mlps_forget_split='first_64_partitioned_unsplit'_inject_fact=True_run_id=1
wandb: ⭐️ View project at https://wandb.ai/quirky_lats_at_mats/circuit_breaking
wandb: 🚀 View run at https://wandb.ai/quirky_lats_at_mats/circuit_breaking/runs/nsqh90gf
Memory at start for localized_ap_mlps: 0.0
==========Partition 0, 0_16==========
Manual param count for partition 0:  704643072
Current partition forget split: first_64_partition_0_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
0       0  ...         0
1      22  ...        22
2      29  ...        29
3      48  ...        48
4      54  ...        54
5      60  ...        60
6      86  ...        86
7     104  ...       104
8     109  ...       109
9     116  ...       116
10    119  ...       119
11    125  ...       125
12    137  ...       137
13    139  ...       139
14    168  ...       168
15    181  ...       181

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'m0': 0.00634765625, 'm1': 0.028076171875, 'm2': 0.08837890625, 'm3': 0.0411376953125, 'm4': 0.011474609375, 'm5': -0.0054779052734375, 'm6': 0.091064453125, 'm7': 0.02294921875, 'm8': 0.0858154296875, 'm9': 0.0481109619140625, 'm10': 0.141845703125, 'm11': 0.081298828125, 'm12': 0.056396484375, 'm13': -0.035888671875, 'm14': -0.018310546875, 'm15': -0.08642578125, 'm16': -0.0096435546875, 'm17': -0.09326171875, 'm18': -0.05377197265625, 'm19': -0.020599365234375, 'm20': -0.0701904296875, 'm21': -0.1904296875, 'm22': -0.05419921875, 'm23': -0.086181640625, 'm24': -0.109619140625, 'm25': -0.03717041015625, 'm26': -0.0292816162109375, 'm27': -0.01611328125, 'm28': -0.0145263671875, 'm29': -0.023101806640625, 'm30': 0.103759765625, 'm31': -0.01123046875}
Using param_count
sorted_attrs=[('m21', -0.1904296875), ('m10', 0.141845703125), ('m24', -0.109619140625), ('m30', 0.103759765625), ('m17', -0.09326171875), ('m6', 0.091064453125), ('m2', 0.08837890625), ('m15', -0.08642578125), ('m23', -0.086181640625), ('m8', 0.0858154296875), ('m11', 0.081298828125), ('m20', -0.0701904296875), ('m12', 0.056396484375), ('m22', -0.05419921875), ('m18', -0.05377197265625), ('m9', 0.0481109619140625), ('m3', 0.0411376953125), ('m25', -0.03717041015625), ('m13', -0.035888671875), ('m26', -0.0292816162109375), ('m1', 0.028076171875), ('m29', -0.023101806640625), ('m7', 0.02294921875), ('m19', -0.020599365234375), ('m14', -0.018310546875), ('m27', -0.01611328125), ('m28', -0.0145263671875), ('m4', 0.011474609375), ('m31', -0.01123046875), ('m16', -0.0096435546875), ('m0', 0.00634765625), ('m5', -0.0054779052734375)]
blocks.21.mlp.hook_gate 528482304
blocks.10.mlp.hook_gate 352321536
blocks.24.mlp.hook_gate 176160768
blocks.30.mlp.hook_gate 0
Thresholding importance at 0.103759765625
component='m10', importance=0.141845703125 is being added
component='m21', importance=0.1904296875 is being added
component='m24', importance=0.109619140625 is being added
component='m30', importance=0.103759765625 is being added
Number of parameters in localized_ap_mlps localization: 704643072
final_components={'blocks.30.mlp.hook_post', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.21.mlp.hook_gate', 'blocks.24.mlp.hook_post', 'blocks.10.mlp.hook_post', 'blocks.24.mlp.hook_gate', 'blocks.30.mlp.hook_gate', 'blocks.24.mlp.hook_pre', 'blocks.30.mlp.hook_pre', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_gate'}
  0%|          | 0/25 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  4%|▍         | 1/25 [00:28<11:35, 28.99s/it]  8%|▊         | 2/25 [00:34<05:43, 14.94s/it] 12%|█▏        | 3/25 [00:39<03:48, 10.40s/it] 16%|█▌        | 4/25 [00:44<02:56,  8.38s/it] 20%|██        | 5/25 [00:49<02:25,  7.25s/it] 24%|██▍       | 6/25 [00:59<02:37,  8.28s/it] 28%|██▊       | 7/25 [01:05<02:10,  7.24s/it] 32%|███▏      | 8/25 [01:10<01:54,  6.72s/it] 36%|███▌      | 9/25 [01:15<01:38,  6.17s/it] 40%|████      | 10/25 [01:20<01:28,  5.89s/it] 44%|████▍     | 11/25 [01:30<01:38,  7.06s/it] 48%|████▊     | 12/25 [01:35<01:24,  6.47s/it] 52%|█████▏    | 13/25 [01:40<01:12,  6.03s/it] 56%|█████▌    | 14/25 [01:45<01:02,  5.72s/it] 60%|██████    | 15/25 [01:50<00:55,  5.59s/it] 64%|██████▍   | 16/25 [02:00<01:01,  6.81s/it] 68%|██████▊   | 17/25 [02:05<00:50,  6.34s/it] 72%|███████▏  | 18/25 [02:11<00:42,  6.09s/it] 76%|███████▌  | 19/25 [02:16<00:35,  5.84s/it] 80%|████████  | 20/25 [02:21<00:28,  5.65s/it] 84%|████████▍ | 21/25 [02:31<00:28,  7.00s/it] 88%|████████▊ | 22/25 [02:37<00:19,  6.50s/it] 92%|█████████▏| 23/25 [02:42<00:12,  6.09s/it] 96%|█████████▌| 24/25 [02:47<00:05,  5.77s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [04:06<00:00, 27.62s/it]100%|██████████| 25/25 [04:06<00:00,  9.84s/it]
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
After epoch, mem is  18.91091251373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  17.59841251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 1, 16_32==========
Manual param count for partition 1:  1056964608
Current partition forget split: first_64_partition_1_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
16    196  ...       196
17    200  ...       200
18    215  ...       215
19    225  ...       225
20    231  ...       231
21    234  ...       234
22    239  ...       239
23    241  ...       241
24    248  ...       248
25    253  ...       253
26    272  ...       272
27    301  ...       301
28    307  ...       307
29    333  ...       333
30    335  ...       335
31    339  ...       339

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'m0': -0.186279296875, 'm1': -0.0283203125, 'm2': 0.0712890625, 'm3': 0.035064697265625, 'm4': -0.081787109375, 'm5': -0.0992431640625, 'm6': -0.049072265625, 'm7': -0.04473876953125, 'm8': 0.01055908203125, 'm9': 0.088134765625, 'm10': 0.056365966796875, 'm11': 0.147705078125, 'm12': 0.0228271484375, 'm13': -0.060791015625, 'm14': -0.005615234375, 'm15': -0.054931640625, 'm16': -0.0093994140625, 'm17': -0.0654296875, 'm18': -0.04486083984375, 'm19': -0.0079345703125, 'm20': -0.06512451171875, 'm21': -0.2138671875, 'm22': -0.0721435546875, 'm23': 0.01532745361328125, 'm24': -0.15380859375, 'm25': -0.034423828125, 'm26': -0.018316268920898438, 'm27': -0.06817626953125, 'm28': 0.03656005859375, 'm29': -0.018524169921875, 'm30': 0.1884765625, 'm31': -0.4248046875}
Using param_count
sorted_attrs=[('m31', -0.4248046875), ('m21', -0.2138671875), ('m30', 0.1884765625), ('m0', -0.186279296875), ('m24', -0.15380859375), ('m11', 0.147705078125), ('m5', -0.0992431640625), ('m9', 0.088134765625), ('m4', -0.081787109375), ('m22', -0.0721435546875), ('m2', 0.0712890625), ('m27', -0.06817626953125), ('m17', -0.0654296875), ('m20', -0.06512451171875), ('m13', -0.060791015625), ('m10', 0.056365966796875), ('m15', -0.054931640625), ('m6', -0.049072265625), ('m18', -0.04486083984375), ('m7', -0.04473876953125), ('m28', 0.03656005859375), ('m3', 0.035064697265625), ('m25', -0.034423828125), ('m1', -0.0283203125), ('m12', 0.0228271484375), ('m29', -0.018524169921875), ('m26', -0.018316268920898438), ('m23', 0.01532745361328125), ('m8', 0.01055908203125), ('m16', -0.0093994140625), ('m19', -0.0079345703125), ('m14', -0.005615234375)]
blocks.31.mlp.hook_gate 880803840
blocks.21.mlp.hook_gate 704643072
blocks.30.mlp.hook_gate 528482304
blocks.0.mlp.hook_gate 352321536
blocks.24.mlp.hook_gate 176160768
blocks.11.mlp.hook_gate 0
Thresholding importance at 0.147705078125
component='m0', importance=0.186279296875 is being added
component='m11', importance=0.147705078125 is being added
component='m21', importance=0.2138671875 is being added
component='m24', importance=0.15380859375 is being added
component='m30', importance=0.1884765625 is being added
component='m31', importance=0.4248046875 is being added
Number of parameters in localized_ap_mlps localization: 1056964608
final_components={'blocks.30.mlp.hook_post', 'blocks.21.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.21.mlp.hook_post', 'blocks.11.mlp.hook_pre', 'blocks.21.mlp.hook_gate', 'blocks.24.mlp.hook_post', 'blocks.31.mlp.hook_pre', 'blocks.31.mlp.hook_gate', 'blocks.0.mlp.hook_pre', 'blocks.30.mlp.hook_gate', 'blocks.0.mlp.hook_gate', 'blocks.11.mlp.hook_gate', 'blocks.31.mlp.hook_post', 'blocks.24.mlp.hook_pre', 'blocks.30.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.24.mlp.hook_gate'}
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:43<17:21, 43.39s/it]  8%|▊         | 2/25 [00:49<08:15, 21.54s/it] 12%|█▏        | 3/25 [00:56<05:23, 14.70s/it] 16%|█▌        | 4/25 [01:02<03:59, 11.43s/it] 20%|██        | 5/25 [01:09<03:12,  9.65s/it] 24%|██▍       | 6/25 [01:20<03:12, 10.13s/it] 28%|██▊       | 7/25 [01:26<02:42,  9.03s/it] 32%|███▏      | 8/25 [01:33<02:19,  8.22s/it] 36%|███▌      | 9/25 [01:40<02:03,  7.74s/it] 40%|████      | 10/25 [01:46<01:50,  7.34s/it] 44%|████▍     | 11/25 [01:57<01:58,  8.45s/it] 48%|████▊     | 12/25 [02:04<01:42,  7.89s/it] 52%|█████▏    | 13/25 [02:10<01:30,  7.55s/it] 56%|█████▌    | 14/25 [02:17<01:19,  7.26s/it] 60%|██████    | 15/25 [02:23<01:10,  7.01s/it] 64%|██████▍   | 16/25 [02:35<01:14,  8.26s/it] 68%|██████▊   | 17/25 [02:41<01:02,  7.80s/it] 72%|███████▏  | 18/25 [02:48<00:52,  7.45s/it] 76%|███████▌  | 19/25 [02:55<00:43,  7.25s/it] 80%|████████  | 20/25 [03:01<00:35,  7.04s/it] 84%|████████▍ | 21/25 [03:54<01:23, 20.78s/it] 88%|████████▊ | 22/25 [04:01<00:49, 16.52s/it] 92%|█████████▏| 23/25 [04:07<00:26, 13.45s/it] 96%|█████████▌| 24/25 [04:13<00:11, 11.35s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [05:32<00:00, 31.55s/it]100%|██████████| 25/25 [05:32<00:00, 13.30s/it]
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
After epoch, mem is  20.87966251373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  18.91091251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 2, 32_48==========
Manual param count for partition 2:  880803840
Current partition forget split: first_64_partition_2_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
32    354  ...       354
33    357  ...       357
34    361  ...       361
35    371  ...       371
36    372  ...       372
37    380  ...       380
38    381  ...       381
39    391  ...       391
40    397  ...       397
41    421  ...       421
42    422  ...       422
43    424  ...       424
44    428  ...       428
45    433  ...       433
46    461  ...       461
47    475  ...       475

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'m0': 0.03424072265625, 'm1': -0.0152587890625, 'm2': -0.052978515625, 'm3': 0.0845947265625, 'm4': -0.06689453125, 'm5': 0.041961669921875, 'm6': -0.059295654296875, 'm7': 0.0775146484375, 'm8': 0.02838134765625, 'm9': 0.14453125, 'm10': 0.103515625, 'm11': 0.04949951171875, 'm12': 0.021728515625, 'm13': -0.05120086669921875, 'm14': 0.0224609375, 'm15': -0.04506683349609375, 'm16': -0.0689697265625, 'm17': -0.05181884765625, 'm18': -0.089111328125, 'm19': 0.0008544921875, 'm20': -0.077392578125, 'm21': -0.21875, 'm22': -0.19482421875, 'm23': -0.0743408203125, 'm24': -0.0531005859375, 'm25': -0.0869140625, 'm26': -0.033599853515625, 'm27': 0.004150390625, 'm28': 0.001953125, 'm29': 0.06597900390625, 'm30': 0.058837890625, 'm31': -0.20458984375}
Using param_count
sorted_attrs=[('m21', -0.21875), ('m31', -0.20458984375), ('m22', -0.19482421875), ('m9', 0.14453125), ('m10', 0.103515625), ('m18', -0.089111328125), ('m25', -0.0869140625), ('m3', 0.0845947265625), ('m7', 0.0775146484375), ('m20', -0.077392578125), ('m23', -0.0743408203125), ('m16', -0.0689697265625), ('m4', -0.06689453125), ('m29', 0.06597900390625), ('m6', -0.059295654296875), ('m30', 0.058837890625), ('m24', -0.0531005859375), ('m2', -0.052978515625), ('m17', -0.05181884765625), ('m13', -0.05120086669921875), ('m11', 0.04949951171875), ('m15', -0.04506683349609375), ('m5', 0.041961669921875), ('m0', 0.03424072265625), ('m26', -0.033599853515625), ('m8', 0.02838134765625), ('m14', 0.0224609375), ('m12', 0.021728515625), ('m1', -0.0152587890625), ('m27', 0.004150390625), ('m28', 0.001953125), ('m19', 0.0008544921875)]
blocks.21.mlp.hook_gate 704643072
blocks.31.mlp.hook_gate 528482304
blocks.22.mlp.hook_gate 352321536
blocks.9.mlp.hook_gate 176160768
blocks.10.mlp.hook_gate 0
Thresholding importance at 0.103515625
component='m9', importance=0.14453125 is being added
component='m10', importance=0.103515625 is being added
component='m21', importance=0.21875 is being added
component='m22', importance=0.19482421875 is being added
component='m31', importance=0.20458984375 is being added
Number of parameters in localized_ap_mlps localization: 880803840
final_components={'blocks.9.mlp.hook_pre', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.22.mlp.hook_pre', 'blocks.21.mlp.hook_gate', 'blocks.31.mlp.hook_pre', 'blocks.31.mlp.hook_gate', 'blocks.10.mlp.hook_post', 'blocks.9.mlp.hook_post', 'blocks.31.mlp.hook_post', 'blocks.9.mlp.hook_gate', 'blocks.10.mlp.hook_pre', 'blocks.22.mlp.hook_post', 'blocks.10.mlp.hook_gate', 'blocks.22.mlp.hook_gate'}
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:22<08:54, 22.27s/it]  8%|▊         | 2/25 [00:27<04:44, 12.36s/it] 12%|█▏        | 3/25 [00:33<03:23,  9.23s/it] 16%|█▌        | 4/25 [00:38<02:40,  7.66s/it] 20%|██        | 5/25 [00:43<02:14,  6.74s/it] 24%|██▍       | 6/25 [00:53<02:31,  7.97s/it] 28%|██▊       | 7/25 [00:59<02:08,  7.13s/it] 32%|███▏      | 8/25 [01:04<01:52,  6.61s/it] 36%|███▌      | 9/25 [01:10<01:39,  6.22s/it] 40%|████      | 10/25 [01:15<01:29,  5.99s/it] 44%|████▍     | 11/25 [01:25<01:40,  7.20s/it] 48%|████▊     | 12/25 [01:30<01:26,  6.63s/it] 52%|█████▏    | 13/25 [01:36<01:15,  6.29s/it] 56%|█████▌    | 14/25 [01:41<01:06,  6.02s/it] 60%|██████    | 15/25 [01:47<00:57,  5.77s/it] 64%|██████▍   | 16/25 [01:56<01:03,  7.01s/it] 68%|██████▊   | 17/25 [02:02<00:51,  6.47s/it] 72%|███████▏  | 18/25 [02:07<00:43,  6.20s/it] 76%|███████▌  | 19/25 [02:13<00:36,  6.05s/it] 80%|████████  | 20/25 [02:18<00:29,  5.82s/it] 84%|████████▍ | 21/25 [02:28<00:27,  6.99s/it] 88%|████████▊ | 22/25 [02:33<00:19,  6.46s/it] 92%|█████████▏| 23/25 [02:38<00:12,  6.13s/it] 96%|█████████▌| 24/25 [02:44<00:05,  5.82s/it]