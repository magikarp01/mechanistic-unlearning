/data/phillip_guo/mechanistic-unlearning
Loading args from config file: results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ct_run1/config.json
==========ARGS==========
Namespace(config_path='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ct_run1/config.json', save_dir='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ct_run1', model_type='llama-3-8b', forget_split='first_64_partitioned_unsplit', inject_fact=True, localization_type='localized_ct', run_id=1, combine_heads=True, train_batch_size=4, eval_batch_size=32, learning_rate=5e-05, grad_accum_steps=16, mixed_precision=False, n_epochs=25, beta=3, clip_grad=1, evaluate_every=5, n_eval_iters=5, deep_evaluate_every=None, do_adversarial_evals=True, n_mc_shots=8, do_side_effects_evals=True, check_all_logits=False, use_wandb=True, save_model=False, push_to_hub=False, do_full_mmlu_evals=True, do_relearning_evals=True, n_relearn_iters=20, n_relearn_facts=32, lora_rank=512, target_modules='all-linear', relearning_lr=0.0002, forget_loss_coef=2.0, do_softprompt_evals=True, softprompt_attack_batch_size=16, num_softprompts=4)
==========END ARGS==========
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
wandb: Currently logged in as: philliphguo (quirky_lats_at_mats). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /data/phillip_guo/mechanistic-unlearning/wandb/run-20241126_082749-xgw4peh1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuning_counterfact_localized_ct_forget_split='first_64_partitioned_unsplit'_inject_fact=True_run_id=1
wandb: ⭐️ View project at https://wandb.ai/quirky_lats_at_mats/circuit_breaking
wandb: 🚀 View run at https://wandb.ai/quirky_lats_at_mats/circuit_breaking/runs/xgw4peh1
Memory at start for localized_ct: 0.0
==========Partition 0, 0_16==========
Manual param count for partition 0:  704643072
Current partition forget split: first_64_partition_0_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
0       0  ...         0
1      22  ...        22
2      29  ...        29
3      48  ...        48
4      54  ...        54
5      60  ...        60
6      86  ...        86
7     104  ...       104
8     109  ...       109
9     116  ...       116
10    119  ...       119
11    125  ...       125
12    137  ...       137
13    139  ...       139
14    168  ...       168
15    181  ...       181

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'a0': -0.6522216796875, 'm0': -0.041259765625, 'a1': -0.7513427734375, 'm1': -0.02880859375, 'a2': -0.852508544921875, 'm2': -0.0206298828125, 'a3': -0.5783214569091797, 'm3': -0.0238037109375, 'a4': -0.913116455078125, 'm4': 0.04345703125, 'a5': -0.7309417724609375, 'm5': 0.0101318359375, 'a6': -0.805694580078125, 'm6': -0.0216064453125, 'a7': -0.742523193359375, 'm7': -0.0198974609375, 'a8': -0.7276611328125, 'm8': -0.0069580078125, 'a9': -0.7370185852050781, 'm9': -0.048095703125, 'a10': -0.680572509765625, 'm10': 0.00091552734375, 'a11': -0.7477264404296875, 'm11': -0.032958984375, 'a12': -0.8939056396484375, 'm12': -0.033447265625, 'a13': -0.6648330688476562, 'm13': -0.028564453125, 'a14': -0.5556411743164062, 'm14': 0.00445556640625, 'a15': -0.6451263427734375, 'm15': 0.005401611328125, 'a16': -0.891265869140625, 'm16': 0.015869140625, 'a17': -0.68658447265625, 'm17': -0.004638671875, 'a18': -0.8380126953125, 'm18': -0.01068115234375, 'a19': -0.817535400390625, 'm19': -0.0179443359375, 'a20': -0.7194366455078125, 'm20': -0.01312255859375, 'a21': -0.7610702514648438, 'm21': -0.005584716796875, 'a22': -0.8335342407226562, 'm22': -0.020751953125, 'a23': -0.892333984375, 'm23': -0.02294921875, 'a24': -0.6141204833984375, 'm24': -0.029541015625, 'a25': -0.78265380859375, 'm25': -0.01361083984375, 'a26': -0.7416534423828125, 'm26': -0.037353515625, 'a27': -0.775146484375, 'm27': -0.00152587890625, 'a28': -0.831329345703125, 'm28': -0.00543212890625, 'a29': -0.815704345703125, 'm29': 0.0159912109375, 'a30': -0.6985015869140625, 'm30': 0.0032806396484375, 'a31': -0.7074661254882812, 'm31': 0.003448486328125}
Using param_count
sorted_attrs=[('a4', -0.913116455078125), ('a12', -0.8939056396484375), ('a23', -0.892333984375), ('a16', -0.891265869140625), ('a2', -0.852508544921875), ('a18', -0.8380126953125), ('a22', -0.8335342407226562), ('a28', -0.831329345703125), ('a19', -0.817535400390625), ('a29', -0.815704345703125), ('a6', -0.805694580078125), ('a25', -0.78265380859375), ('a27', -0.775146484375), ('a21', -0.7610702514648438), ('a1', -0.7513427734375), ('a11', -0.7477264404296875), ('a7', -0.742523193359375), ('a26', -0.7416534423828125), ('a9', -0.7370185852050781), ('a5', -0.7309417724609375), ('a8', -0.7276611328125), ('a20', -0.7194366455078125), ('a31', -0.7074661254882812), ('a30', -0.6985015869140625), ('a17', -0.68658447265625), ('a10', -0.680572509765625), ('a13', -0.6648330688476562), ('a0', -0.6522216796875), ('a15', -0.6451263427734375), ('a24', -0.6141204833984375), ('a3', -0.5783214569091797), ('a14', -0.5556411743164062), ('m9', -0.048095703125), ('m4', 0.04345703125), ('m0', -0.041259765625), ('m26', -0.037353515625), ('m12', -0.033447265625), ('m11', -0.032958984375), ('m24', -0.029541015625), ('m1', -0.02880859375), ('m13', -0.028564453125), ('m3', -0.0238037109375), ('m23', -0.02294921875), ('m6', -0.0216064453125), ('m22', -0.020751953125), ('m2', -0.0206298828125), ('m7', -0.0198974609375), ('m19', -0.0179443359375), ('m29', 0.0159912109375), ('m16', 0.015869140625), ('m25', -0.01361083984375), ('m20', -0.01312255859375), ('m18', -0.01068115234375), ('m5', 0.0101318359375), ('m8', -0.0069580078125), ('m21', -0.005584716796875), ('m28', -0.00543212890625), ('m15', 0.005401611328125), ('m17', -0.004638671875), ('m14', 0.00445556640625), ('m31', 0.003448486328125), ('m30', 0.0032806396484375), ('m27', -0.00152587890625), ('m10', 0.00091552734375)]
blocks.4.attn.hook_result 662700032
blocks.12.attn.hook_result 620756992
blocks.23.attn.hook_result 578813952
blocks.16.attn.hook_result 536870912
blocks.2.attn.hook_result 494927872
blocks.18.attn.hook_result 452984832
blocks.22.attn.hook_result 411041792
blocks.28.attn.hook_result 369098752
blocks.19.attn.hook_result 327155712
blocks.29.attn.hook_result 285212672
blocks.6.attn.hook_result 243269632
blocks.25.attn.hook_result 201326592
blocks.27.attn.hook_result 159383552
blocks.21.attn.hook_result 117440512
blocks.1.attn.hook_result 75497472
blocks.11.attn.hook_result 33554432
blocks.7.attn.hook_result -8388608
Thresholding importance at 0.742523193359375
component='a1', importance=0.7513427734375 is being added
component='a2', importance=0.852508544921875 is being added
component='a4', importance=0.913116455078125 is being added
component='a6', importance=0.805694580078125 is being added
component='a7', importance=0.742523193359375 is being added
component='a11', importance=0.7477264404296875 is being added
component='a12', importance=0.8939056396484375 is being added
component='a16', importance=0.891265869140625 is being added
component='a18', importance=0.8380126953125 is being added
component='a19', importance=0.817535400390625 is being added
component='a21', importance=0.7610702514648438 is being added
component='a22', importance=0.8335342407226562 is being added
component='a23', importance=0.892333984375 is being added
component='a25', importance=0.78265380859375 is being added
component='a27', importance=0.775146484375 is being added
component='a28', importance=0.831329345703125 is being added
component='a29', importance=0.815704345703125 is being added
Number of parameters in localized_ct localization: 713031680
final_components={'blocks.4.attn.hook_v', 'blocks.23.attn.hook_k', 'blocks.25.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.7.attn.hook_k', 'blocks.18.attn.hook_result', 'blocks.22.attn.hook_k', 'blocks.27.attn.hook_q', 'blocks.29.attn.hook_k', 'blocks.29.attn.hook_result', 'blocks.19.attn.hook_result', 'blocks.21.attn.hook_result', 'blocks.25.attn.hook_v', 'blocks.19.attn.hook_q', 'blocks.11.attn.hook_v', 'blocks.22.attn.hook_v', 'blocks.11.attn.hook_q', 'blocks.12.attn.hook_result', 'blocks.19.attn.hook_k', 'blocks.2.attn.hook_q', 'blocks.16.attn.hook_v', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.1.attn.hook_result', 'blocks.23.attn.hook_v', 'blocks.27.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.7.attn.hook_q', 'blocks.4.attn.hook_result', 'blocks.11.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.6.attn.hook_v', 'blocks.28.attn.hook_v', 'blocks.22.attn.hook_result', 'blocks.6.attn.hook_result', 'blocks.11.attn.hook_result', 'blocks.1.attn.hook_k', 'blocks.6.attn.hook_k', 'blocks.23.attn.hook_result', 'blocks.23.attn.hook_q', 'blocks.29.attn.hook_v', 'blocks.27.attn.hook_result', 'blocks.29.attn.hook_q', 'blocks.4.attn.hook_q', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.21.attn.hook_k', 'blocks.4.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.25.attn.hook_result', 'blocks.21.attn.hook_q', 'blocks.2.attn.hook_v', 'blocks.7.attn.hook_result', 'blocks.27.attn.hook_v', 'blocks.2.attn.hook_result', 'blocks.25.attn.hook_k', 'blocks.28.attn.hook_result', 'blocks.1.attn.hook_q', 'blocks.6.attn.hook_q', 'blocks.28.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.28.attn.hook_q', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_result', 'blocks.16.attn.hook_k', 'blocks.22.attn.hook_q', 'blocks.21.attn.hook_v'}
  0%|          | 0/25 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  4%|▍         | 1/25 [00:27<10:48, 27.00s/it]  8%|▊         | 2/25 [00:32<05:34, 14.56s/it] 12%|█▏        | 3/25 [00:38<03:54, 10.66s/it] 16%|█▌        | 4/25 [00:44<03:03,  8.74s/it] 20%|██        | 5/25 [00:50<02:33,  7.68s/it] 24%|██▍       | 6/25 [01:01<02:44,  8.65s/it] 28%|██▊       | 7/25 [01:06<02:19,  7.77s/it] 32%|███▏      | 8/25 [01:12<02:01,  7.15s/it] 36%|███▌      | 9/25 [01:18<01:47,  6.73s/it] 40%|████      | 10/25 [01:24<01:36,  6.46s/it] 44%|████▍     | 11/25 [01:34<01:47,  7.66s/it] 48%|████▊     | 12/25 [01:40<01:33,  7.18s/it] 52%|█████▏    | 13/25 [01:46<01:21,  6.79s/it] 56%|█████▌    | 14/25 [01:52<01:12,  6.55s/it] 60%|██████    | 15/25 [01:58<01:03,  6.32s/it] 64%|██████▍   | 16/25 [02:09<01:10,  7.84s/it] 68%|██████▊   | 17/25 [02:16<00:59,  7.40s/it] 72%|███████▏  | 18/25 [02:22<00:48,  6.92s/it] 76%|███████▌  | 19/25 [02:27<00:39,  6.58s/it] 80%|████████  | 20/25 [02:33<00:31,  6.34s/it] 84%|████████▍ | 21/25 [02:44<00:30,  7.59s/it] 88%|████████▊ | 22/25 [02:50<00:21,  7.10s/it] 92%|█████████▏| 23/25 [02:56<00:13,  6.89s/it] 96%|█████████▌| 24/25 [03:02<00:06,  6.57s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [04:21<00:00, 28.25s/it]100%|██████████| 25/25 [04:21<00:00, 10.45s/it]
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
After epoch, mem is  18.95778751373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  17.62966251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 1, 16_32==========
Manual param count for partition 1:  1056964608
Current partition forget split: first_64_partition_1_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
16    196  ...       196
17    200  ...       200
18    215  ...       215
19    225  ...       225
20    231  ...       231
21    234  ...       234
22    239  ...       239
23    241  ...       241
24    248  ...       248
25    253  ...       253
26    272  ...       272
27    301  ...       301
28    307  ...       307
29    333  ...       333
30    335  ...       335
31    339  ...       339

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'a0': -0.20660400390625, 'm0': 0.0296630859375, 'a1': -0.4494438171386719, 'm1': 0.0301513671875, 'a2': -0.24121856689453125, 'm2': -0.009765625, 'a3': -0.37139892578125, 'm3': 0.017333984375, 'a4': -0.440887451171875, 'm4': -0.01470947265625, 'a5': -0.5267806053161621, 'm5': 0.040283203125, 'a6': -0.2682981491088867, 'm6': -0.01165771484375, 'a7': -0.301910400390625, 'm7': 0.0174560546875, 'a8': -0.352081298828125, 'm8': 0.00144195556640625, 'a9': -0.496612548828125, 'm9': -0.02783203125, 'a10': -0.32943153381347656, 'm10': 0.00032806396484375, 'a11': -0.3038825988769531, 'm11': 0.0113525390625, 'a12': -0.299346923828125, 'm12': -0.035400390625, 'a13': -0.37114715576171875, 'm13': 0.026611328125, 'a14': 0.13006591796875, 'm14': 0.01055908203125, 'a15': -0.14423370361328125, 'm15': 0.06884765625, 'a16': -0.5052986145019531, 'm16': 0.00347900390625, 'a17': -0.2115478515625, 'm17': -0.004791259765625, 'a18': -0.3647880554199219, 'm18': -0.0174560546875, 'a19': -0.3460235595703125, 'm19': -0.028076171875, 'a20': -0.35662078857421875, 'm20': 0.11767578125, 'a21': -0.3677101135253906, 'm21': 0.0169677734375, 'a22': 0.01309967041015625, 'm22': 0.01409912109375, 'a23': -0.36363983154296875, 'm23': -0.00701904296875, 'a24': -0.32605743408203125, 'm24': -0.017578125, 'a25': -0.2358551025390625, 'm25': 0.048583984375, 'a26': -0.350006103515625, 'm26': -0.0128173828125, 'a27': -0.5738029479980469, 'm27': -0.00067138671875, 'a28': -0.37652587890625, 'm28': -0.0238037109375, 'a29': -0.511993408203125, 'm29': -0.01324462890625, 'a30': -0.359893798828125, 'm30': -0.0017547607421875, 'a31': -0.23895645141601562, 'm31': -0.00164031982421875}
Using param_count
sorted_attrs=[('a27', -0.5738029479980469), ('a5', -0.5267806053161621), ('a29', -0.511993408203125), ('a16', -0.5052986145019531), ('a9', -0.496612548828125), ('a1', -0.4494438171386719), ('a4', -0.440887451171875), ('a28', -0.37652587890625), ('a3', -0.37139892578125), ('a13', -0.37114715576171875), ('a21', -0.3677101135253906), ('a18', -0.3647880554199219), ('a23', -0.36363983154296875), ('a30', -0.359893798828125), ('a20', -0.35662078857421875), ('a8', -0.352081298828125), ('a26', -0.350006103515625), ('a19', -0.3460235595703125), ('a10', -0.32943153381347656), ('a24', -0.32605743408203125), ('a11', -0.3038825988769531), ('a7', -0.301910400390625), ('a12', -0.299346923828125), ('a6', -0.2682981491088867), ('a2', -0.24121856689453125), ('a31', -0.23895645141601562), ('a25', -0.2358551025390625), ('a17', -0.2115478515625), ('a0', -0.20660400390625), ('a15', -0.14423370361328125), ('a14', 0.13006591796875), ('m20', 0.11767578125), ('m15', 0.06884765625), ('m25', 0.048583984375), ('m5', 0.040283203125), ('m12', -0.035400390625), ('m1', 0.0301513671875), ('m0', 0.0296630859375), ('m19', -0.028076171875), ('m9', -0.02783203125), ('m13', 0.026611328125), ('m28', -0.0238037109375), ('m24', -0.017578125), ('m7', 0.0174560546875), ('m18', -0.0174560546875), ('m3', 0.017333984375), ('m21', 0.0169677734375), ('m4', -0.01470947265625), ('m22', 0.01409912109375), ('m29', -0.01324462890625), ('a22', 0.01309967041015625), ('m26', -0.0128173828125), ('m6', -0.01165771484375), ('m11', 0.0113525390625), ('m14', 0.01055908203125), ('m2', -0.009765625), ('m23', -0.00701904296875), ('m17', -0.004791259765625), ('m16', 0.00347900390625), ('m30', -0.0017547607421875), ('m31', -0.00164031982421875), ('m8', 0.00144195556640625), ('m27', -0.00067138671875), ('m10', 0.00032806396484375)]
blocks.27.attn.hook_result 1015021568
blocks.5.attn.hook_result 973078528
blocks.29.attn.hook_result 931135488
blocks.16.attn.hook_result 889192448
blocks.9.attn.hook_result 847249408
blocks.1.attn.hook_result 805306368
blocks.4.attn.hook_result 763363328
blocks.28.attn.hook_result 721420288
blocks.3.attn.hook_result 679477248
blocks.13.attn.hook_result 637534208
blocks.21.attn.hook_result 595591168
blocks.18.attn.hook_result 553648128
blocks.23.attn.hook_result 511705088
blocks.30.attn.hook_result 469762048
blocks.20.attn.hook_result 427819008
blocks.8.attn.hook_result 385875968
blocks.26.attn.hook_result 343932928
blocks.19.attn.hook_result 301989888
blocks.10.attn.hook_result 260046848
blocks.24.attn.hook_result 218103808
blocks.11.attn.hook_result 176160768
blocks.7.attn.hook_result 134217728
blocks.12.attn.hook_result 92274688
blocks.6.attn.hook_result 50331648
blocks.2.attn.hook_result 8388608
blocks.31.attn.hook_result -33554432
Thresholding importance at 0.23895645141601562
component='a1', importance=0.4494438171386719 is being added
component='a2', importance=0.24121856689453125 is being added
component='a3', importance=0.37139892578125 is being added
component='a4', importance=0.440887451171875 is being added
component='a5', importance=0.5267806053161621 is being added
component='a6', importance=0.2682981491088867 is being added
component='a7', importance=0.301910400390625 is being added
component='a8', importance=0.352081298828125 is being added
component='a9', importance=0.496612548828125 is being added
component='a10', importance=0.32943153381347656 is being added
component='a11', importance=0.3038825988769531 is being added
component='a12', importance=0.299346923828125 is being added
component='a13', importance=0.37114715576171875 is being added
component='a16', importance=0.5052986145019531 is being added
component='a18', importance=0.3647880554199219 is being added
component='a19', importance=0.3460235595703125 is being added
component='a20', importance=0.35662078857421875 is being added
component='a21', importance=0.3677101135253906 is being added
component='a23', importance=0.36363983154296875 is being added
component='a24', importance=0.32605743408203125 is being added
component='a26', importance=0.350006103515625 is being added
component='a27', importance=0.5738029479980469 is being added
component='a28', importance=0.37652587890625 is being added
component='a29', importance=0.511993408203125 is being added
component='a30', importance=0.359893798828125 is being added
component='a31', importance=0.23895645141601562 is being added
Number of parameters in localized_ct localization: 1090519040
final_components={'blocks.7.attn.hook_k', 'blocks.29.attn.hook_k', 'blocks.31.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.12.attn.hook_result', 'blocks.31.attn.hook_q', 'blocks.3.attn.hook_v', 'blocks.1.attn.hook_result', 'blocks.27.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_result', 'blocks.11.attn.hook_result', 'blocks.29.attn.hook_v', 'blocks.29.attn.hook_q', 'blocks.12.attn.hook_v', 'blocks.21.attn.hook_q', 'blocks.7.attn.hook_result', 'blocks.1.attn.hook_q', 'blocks.28.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.16.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.29.attn.hook_result', 'blocks.24.attn.hook_k', 'blocks.10.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.8.attn.hook_k', 'blocks.19.attn.hook_k', 'blocks.30.attn.hook_v', 'blocks.2.attn.hook_q', 'blocks.26.attn.hook_q', 'blocks.23.attn.hook_v', 'blocks.4.attn.hook_result', 'blocks.7.attn.hook_v', 'blocks.3.attn.hook_q', 'blocks.26.attn.hook_k', 'blocks.12.attn.hook_k', 'blocks.30.attn.hook_result', 'blocks.4.attn.hook_k', 'blocks.30.attn.hook_k', 'blocks.20.attn.hook_result', 'blocks.20.attn.hook_k', 'blocks.9.attn.hook_k', 'blocks.27.attn.hook_v', 'blocks.28.attn.hook_result', 'blocks.6.attn.hook_q', 'blocks.8.attn.hook_result', 'blocks.16.attn.hook_result', 'blocks.5.attn.hook_result', 'blocks.21.attn.hook_v', 'blocks.2.attn.hook_k', 'blocks.27.attn.hook_q', 'blocks.21.attn.hook_result', 'blocks.24.attn.hook_result', 'blocks.10.attn.hook_v', 'blocks.9.attn.hook_q', 'blocks.11.attn.hook_q', 'blocks.20.attn.hook_q', 'blocks.24.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.1.attn.hook_v', 'blocks.28.attn.hook_v', 'blocks.31.attn.hook_v', 'blocks.10.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.6.attn.hook_k', 'blocks.23.attn.hook_result', 'blocks.27.attn.hook_result', 'blocks.4.attn.hook_q', 'blocks.12.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.26.attn.hook_v', 'blocks.8.attn.hook_q', 'blocks.31.attn.hook_result', 'blocks.18.attn.hook_v', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.9.attn.hook_result', 'blocks.9.attn.hook_v', 'blocks.16.attn.hook_k', 'blocks.26.attn.hook_result', 'blocks.13.attn.hook_result', 'blocks.30.attn.hook_q', 'blocks.3.attn.hook_result', 'blocks.23.attn.hook_k', 'blocks.18.attn.hook_result', 'blocks.19.attn.hook_result', 'blocks.19.attn.hook_q', 'blocks.16.attn.hook_v', 'blocks.18.attn.hook_q', 'blocks.13.attn.hook_q', 'blocks.7.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.23.attn.hook_q', 'blocks.10.attn.hook_result', 'blocks.5.attn.hook_k', 'blocks.2.attn.hook_result', 'blocks.5.attn.hook_q', 'blocks.28.attn.hook_q', 'blocks.8.attn.hook_v', 'blocks.20.attn.hook_v'}
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:23<09:30, 23.79s/it]  8%|▊         | 2/25 [00:29<05:08, 13.41s/it] 12%|█▏        | 3/25 [00:36<03:41, 10.08s/it] 16%|█▌        | 4/25 [00:42<02:59,  8.53s/it] 20%|██        | 5/25 [00:48<02:33,  7.65s/it] 24%|██▍       | 6/25 [00:59<02:48,  8.89s/it] 28%|██▊       | 7/25 [01:05<02:23,  7.99s/it] 32%|███▏      | 8/25 [01:11<02:05,  7.39s/it] 36%|███▌      | 9/25 [01:17<01:51,  6.99s/it] 40%|████      | 10/25 [01:24<01:41,  6.79s/it] 44%|████▍     | 11/25 [01:34<01:51,  7.95s/it] 48%|████▊     | 12/25 [01:41<01:36,  7.43s/it] 52%|█████▏    | 13/25 [01:47<01:24,  7.05s/it] 56%|█████▌    | 14/25 [01:53<01:14,  6.74s/it] 60%|██████    | 15/25 [01:59<01:05,  6.57s/it] 64%|██████▍   | 16/25 [02:10<01:10,  7.81s/it] 68%|██████▊   | 17/25 [02:16<00:58,  7.34s/it] 72%|███████▏  | 18/25 [02:23<00:49,  7.13s/it]