/data/phillip_guo/mechanistic-unlearning
Loading args from config file: results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ap_run1/config.json
==========ARGS==========
Namespace(config_path='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ap_run1/config.json', save_dir='results_rebuttal_counterfact/llama3_forget_64_inject_sequential/localized_ap_run1', model_type='llama-3-8b', forget_split='first_64_partitioned_unsplit', inject_fact=True, localization_type='localized_ap', run_id=1, combine_heads=True, train_batch_size=4, eval_batch_size=32, learning_rate=5e-05, grad_accum_steps=16, mixed_precision=False, n_epochs=25, beta=3, clip_grad=1, evaluate_every=5, n_eval_iters=5, deep_evaluate_every=None, do_adversarial_evals=True, n_mc_shots=8, do_side_effects_evals=True, check_all_logits=False, use_wandb=True, save_model=False, push_to_hub=False, do_full_mmlu_evals=True, do_relearning_evals=True, n_relearn_iters=20, n_relearn_facts=32, lora_rank=512, target_modules='all-linear', relearning_lr=0.0002, forget_loss_coef=2.0, do_softprompt_evals=True, softprompt_attack_batch_size=16, num_softprompts=4)
==========END ARGS==========
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.53it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
wandb: Currently logged in as: philliphguo (quirky_lats_at_mats). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /data/phillip_guo/mechanistic-unlearning/wandb/run-20241126_082014-6g8whigr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuning_counterfact_localized_ap_forget_split='first_64_partitioned_unsplit'_inject_fact=True_run_id=1
wandb: ⭐️ View project at https://wandb.ai/quirky_lats_at_mats/circuit_breaking
wandb: 🚀 View run at https://wandb.ai/quirky_lats_at_mats/circuit_breaking/runs/6g8whigr
Memory at start for localized_ap: 0.0
==========Partition 0, 0_16==========
Manual param count for partition 0:  704643072
Current partition forget split: first_64_partition_0_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
0       0  ...         0
1      22  ...        22
2      29  ...        29
3      48  ...        48
4      54  ...        54
5      60  ...        60
6      86  ...        86
7     104  ...       104
8     109  ...       109
9     116  ...       116
10    119  ...       119
11    125  ...       125
12    137  ...       137
13    139  ...       139
14    168  ...       168
15    181  ...       181

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'a0': -0.026919483581878012, 'm0': 0.00634765625, 'a1': -0.12911128997802734, 'm1': 0.028076171875, 'a2': -0.022436991333961487, 'm2': 0.08837890625, 'a3': 0.14658737182617188, 'm3': 0.0411376953125, 'a4': -0.053391218185424805, 'm4': 0.011474609375, 'a5': 0.004152059555053711, 'm5': -0.0054779052734375, 'a6': 0.03143715858459473, 'm6': 0.091064453125, 'a7': -0.0031812191009521484, 'm7': 0.02294921875, 'a8': 0.034249305725097656, 'm8': 0.0858154296875, 'a9': -0.06466460227966309, 'm9': 0.0481109619140625, 'a10': -0.030664563179016113, 'm10': 0.141845703125, 'a11': 0.02253103256225586, 'm11': 0.081298828125, 'a12': -0.05313044786453247, 'm12': 0.056396484375, 'a13': -0.1259150505065918, 'm13': -0.035888671875, 'a14': -0.10650265216827393, 'm14': -0.018310546875, 'a15': -0.05275571346282959, 'm15': -0.08642578125, 'a16': -0.09462349116802216, 'm16': -0.0096435546875, 'a17': -0.16107530891895294, 'm17': -0.09326171875, 'a18': -0.05640761088579893, 'm18': -0.05377197265625, 'a19': -0.03891681553795934, 'm19': -0.020599365234375, 'a20': 0.015780240297317505, 'm20': -0.0701904296875, 'a21': -0.07359079085290432, 'm21': -0.1904296875, 'a22': -0.046109795570373535, 'm22': -0.05419921875, 'a23': 0.005084037780761719, 'm23': -0.086181640625, 'a24': -0.1979764699935913, 'm24': -0.109619140625, 'a25': -0.0043143630027771, 'm25': -0.03717041015625, 'a26': 0.04681379348039627, 'm26': -0.0292816162109375, 'a27': -0.02731156349182129, 'm27': -0.01611328125, 'a28': 0.03318636864423752, 'm28': -0.0145263671875, 'a29': 0.034917980432510376, 'm29': -0.023101806640625, 'a30': -0.005050539970397949, 'm30': 0.103759765625, 'a31': -0.3541110157966614, 'm31': -0.01123046875}
Using param_count
sorted_attrs=[('a31', -0.3541110157966614), ('a24', -0.1979764699935913), ('m21', -0.1904296875), ('a17', -0.16107530891895294), ('a3', 0.14658737182617188), ('m10', 0.141845703125), ('a1', -0.12911128997802734), ('a13', -0.1259150505065918), ('m24', -0.109619140625), ('a14', -0.10650265216827393), ('m30', 0.103759765625), ('a16', -0.09462349116802216), ('m17', -0.09326171875), ('m6', 0.091064453125), ('m2', 0.08837890625), ('m15', -0.08642578125), ('m23', -0.086181640625), ('m8', 0.0858154296875), ('m11', 0.081298828125), ('a21', -0.07359079085290432), ('m20', -0.0701904296875), ('a9', -0.06466460227966309), ('a18', -0.05640761088579893), ('m12', 0.056396484375), ('m22', -0.05419921875), ('m18', -0.05377197265625), ('a4', -0.053391218185424805), ('a12', -0.05313044786453247), ('a15', -0.05275571346282959), ('m9', 0.0481109619140625), ('a26', 0.04681379348039627), ('a22', -0.046109795570373535), ('m3', 0.0411376953125), ('a19', -0.03891681553795934), ('m25', -0.03717041015625), ('m13', -0.035888671875), ('a29', 0.034917980432510376), ('a8', 0.034249305725097656), ('a28', 0.03318636864423752), ('a6', 0.03143715858459473), ('a10', -0.030664563179016113), ('m26', -0.0292816162109375), ('m1', 0.028076171875), ('a27', -0.02731156349182129), ('a0', -0.026919483581878012), ('m29', -0.023101806640625), ('m7', 0.02294921875), ('a11', 0.02253103256225586), ('a2', -0.022436991333961487), ('m19', -0.020599365234375), ('m14', -0.018310546875), ('m27', -0.01611328125), ('a20', 0.015780240297317505), ('m28', -0.0145263671875), ('m4', 0.011474609375), ('m31', -0.01123046875), ('m16', -0.0096435546875), ('m0', 0.00634765625), ('m5', -0.0054779052734375), ('a23', 0.005084037780761719), ('a30', -0.005050539970397949), ('a25', -0.0043143630027771), ('a5', 0.004152059555053711), ('a7', -0.0031812191009521484)]
blocks.31.attn.hook_result 662700032
blocks.24.attn.hook_result 620756992
blocks.21.mlp.hook_gate 444596224
blocks.17.attn.hook_result 402653184
blocks.3.attn.hook_result 360710144
blocks.10.mlp.hook_gate 184549376
blocks.1.attn.hook_result 142606336
blocks.13.attn.hook_result 100663296
blocks.24.mlp.hook_gate -75497472
Thresholding importance at 0.109619140625
component='a1', importance=0.12911128997802734 is being added
component='a3', importance=0.14658737182617188 is being added
component='m10', importance=0.141845703125 is being added
component='a13', importance=0.1259150505065918 is being added
component='a17', importance=0.16107530891895294 is being added
component='m21', importance=0.1904296875 is being added
component='a24', importance=0.1979764699935913 is being added
component='m24', importance=0.109619140625 is being added
component='a31', importance=0.3541110157966614 is being added
Number of parameters in localized_ap localization: 780140544
final_components={'blocks.13.attn.hook_q', 'blocks.31.attn.hook_result', 'blocks.31.attn.hook_v', 'blocks.10.mlp.hook_gate', 'blocks.24.attn.hook_k', 'blocks.1.attn.hook_q', 'blocks.24.attn.hook_q', 'blocks.21.mlp.hook_gate', 'blocks.3.attn.hook_result', 'blocks.21.mlp.hook_post', 'blocks.10.mlp.hook_pre', 'blocks.17.attn.hook_result', 'blocks.13.attn.hook_result', 'blocks.24.mlp.hook_post', 'blocks.21.mlp.hook_pre', 'blocks.13.attn.hook_k', 'blocks.1.attn.hook_result', 'blocks.13.attn.hook_v', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_gate', 'blocks.3.attn.hook_k', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.3.attn.hook_v', 'blocks.17.attn.hook_q', 'blocks.10.mlp.hook_post', 'blocks.3.attn.hook_q', 'blocks.24.attn.hook_result', 'blocks.31.attn.hook_q', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.31.attn.hook_k'}
  0%|          | 0/25 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  4%|▍         | 1/25 [00:31<12:28, 31.18s/it]  8%|▊         | 2/25 [00:37<06:17, 16.41s/it] 12%|█▏        | 3/25 [00:43<04:14, 11.58s/it] 16%|█▌        | 4/25 [00:48<03:15,  9.31s/it] 20%|██        | 5/25 [00:54<02:41,  8.06s/it] 24%|██▍       | 6/25 [01:04<02:47,  8.80s/it] 28%|██▊       | 7/25 [01:11<02:22,  7.91s/it] 32%|███▏      | 8/25 [01:16<02:03,  7.24s/it] 36%|███▌      | 9/25 [01:22<01:48,  6.79s/it] 40%|████      | 10/25 [01:28<01:37,  6.47s/it] 44%|████▍     | 11/25 [01:39<01:51,  7.99s/it] 48%|████▊     | 12/25 [01:45<01:36,  7.39s/it] 52%|█████▏    | 13/25 [01:51<01:22,  6.91s/it] 56%|█████▌    | 14/25 [01:57<01:12,  6.60s/it] 60%|██████    | 15/25 [02:03<01:03,  6.34s/it] 64%|██████▍   | 16/25 [02:13<01:08,  7.62s/it] 68%|██████▊   | 17/25 [02:19<00:56,  7.10s/it] 72%|███████▏  | 18/25 [02:25<00:47,  6.72s/it] 76%|███████▌  | 19/25 [02:31<00:39,  6.57s/it] 80%|████████  | 20/25 [02:37<00:31,  6.34s/it] 84%|████████▍ | 21/25 [02:47<00:30,  7.54s/it] 88%|████████▊ | 22/25 [02:53<00:21,  7.04s/it] 92%|█████████▏| 23/25 [02:59<00:13,  6.67s/it] 96%|█████████▌| 24/25 [03:05<00:06,  6.42s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [04:25<00:00, 28.36s/it]100%|██████████| 25/25 [04:25<00:00, 10.60s/it]
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
After epoch, mem is  19.33278751373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  17.87966251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 1, 16_32==========
Manual param count for partition 1:  1056964608
Current partition forget split: first_64_partition_1_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
16    196  ...       196
17    200  ...       200
18    215  ...       215
19    225  ...       225
20    231  ...       231
21    234  ...       234
22    239  ...       239
23    241  ...       241
24    248  ...       248
25    253  ...       253
26    272  ...       272
27    301  ...       301
28    307  ...       307
29    333  ...       333
30    335  ...       335
31    339  ...       339

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'a0': -0.17990865572937764, 'm0': -0.186279296875, 'a1': -0.3269033432006836, 'm1': -0.0283203125, 'a2': -0.08895933628082275, 'm2': 0.0712890625, 'a3': -0.03866994380950928, 'm3': 0.035064697265625, 'a4': 0.08672085404396057, 'm4': -0.081787109375, 'a5': -0.023745059967041016, 'm5': -0.0992431640625, 'a6': 0.03921997547149658, 'm6': -0.049072265625, 'a7': -0.039597392082214355, 'm7': -0.04473876953125, 'a8': 0.02542567253112793, 'm8': 0.01055908203125, 'a9': -0.04279530048370361, 'm9': 0.088134765625, 'a10': -0.00031697750091552734, 'm10': 0.056365966796875, 'a11': -0.08242678642272949, 'm11': 0.147705078125, 'a12': -0.04239392280578613, 'm12': 0.0228271484375, 'a13': -0.1955568790435791, 'm13': -0.060791015625, 'a14': -0.12910565733909607, 'm14': -0.005615234375, 'a15': -0.0009199380874633789, 'm15': -0.054931640625, 'a16': -0.1073307991027832, 'm16': -0.0093994140625, 'a17': -0.14059853553771973, 'm17': -0.0654296875, 'a18': -0.07619798183441162, 'm18': -0.04486083984375, 'a19': -0.05435308814048767, 'm19': -0.0079345703125, 'a20': 0.004691474139690399, 'm20': -0.06512451171875, 'a21': -0.0434342622756958, 'm21': -0.2138671875, 'a22': -0.04075908660888672, 'm22': -0.0721435546875, 'a23': -0.013729214668273926, 'm23': 0.01532745361328125, 'a24': -0.17723042890429497, 'm24': -0.15380859375, 'a25': 0.0009636878967285156, 'm25': -0.034423828125, 'a26': -0.006070613861083984, 'm26': -0.018316268920898438, 'a27': -0.022157393395900726, 'm27': -0.06817626953125, 'a28': 0.003330707550048828, 'm28': 0.03656005859375, 'a29': -0.006275266408920288, 'm29': -0.018524169921875, 'a30': 0.007513523101806641, 'm30': 0.1884765625, 'a31': -0.47540295124053955, 'm31': -0.4248046875}
Using param_count
sorted_attrs=[('a31', -0.47540295124053955), ('m31', -0.4248046875), ('a1', -0.3269033432006836), ('m21', -0.2138671875), ('a13', -0.1955568790435791), ('m30', 0.1884765625), ('m0', -0.186279296875), ('a0', -0.17990865572937764), ('a24', -0.17723042890429497), ('m24', -0.15380859375), ('m11', 0.147705078125), ('a17', -0.14059853553771973), ('a14', -0.12910565733909607), ('a16', -0.1073307991027832), ('m5', -0.0992431640625), ('a2', -0.08895933628082275), ('m9', 0.088134765625), ('a4', 0.08672085404396057), ('a11', -0.08242678642272949), ('m4', -0.081787109375), ('a18', -0.07619798183441162), ('m22', -0.0721435546875), ('m2', 0.0712890625), ('m27', -0.06817626953125), ('m17', -0.0654296875), ('m20', -0.06512451171875), ('m13', -0.060791015625), ('m10', 0.056365966796875), ('m15', -0.054931640625), ('a19', -0.05435308814048767), ('m6', -0.049072265625), ('m18', -0.04486083984375), ('m7', -0.04473876953125), ('a21', -0.0434342622756958), ('a9', -0.04279530048370361), ('a12', -0.04239392280578613), ('a22', -0.04075908660888672), ('a7', -0.039597392082214355), ('a6', 0.03921997547149658), ('a3', -0.03866994380950928), ('m28', 0.03656005859375), ('m3', 0.035064697265625), ('m25', -0.034423828125), ('m1', -0.0283203125), ('a8', 0.02542567253112793), ('a5', -0.023745059967041016), ('m12', 0.0228271484375), ('a27', -0.022157393395900726), ('m29', -0.018524169921875), ('m26', -0.018316268920898438), ('m23', 0.01532745361328125), ('a23', -0.013729214668273926), ('m8', 0.01055908203125), ('m16', -0.0093994140625), ('m19', -0.0079345703125), ('a30', 0.007513523101806641), ('a29', -0.006275266408920288), ('a26', -0.006070613861083984), ('m14', -0.005615234375), ('a20', 0.004691474139690399), ('a28', 0.003330707550048828), ('a25', 0.0009636878967285156), ('a15', -0.0009199380874633789), ('a10', -0.00031697750091552734)]
blocks.31.attn.hook_result 1015021568
blocks.31.mlp.hook_gate 838860800
blocks.1.attn.hook_result 796917760
blocks.21.mlp.hook_gate 620756992
blocks.13.attn.hook_result 578813952
blocks.30.mlp.hook_gate 402653184
blocks.0.mlp.hook_gate 226492416
blocks.0.attn.hook_result 184549376
blocks.24.attn.hook_result 142606336
blocks.24.mlp.hook_gate -33554432
Thresholding importance at 0.15380859375
component='a0', importance=0.17990865572937764 is being added
component='m0', importance=0.186279296875 is being added
component='a1', importance=0.3269033432006836 is being added
component='a13', importance=0.1955568790435791 is being added
component='m21', importance=0.2138671875 is being added
component='a24', importance=0.17723042890429497 is being added
component='m24', importance=0.15380859375 is being added
component='m30', importance=0.1884765625 is being added
component='a31', importance=0.47540295124053955 is being added
component='m31', importance=0.4248046875 is being added
Number of parameters in localized_ap localization: 1090519040
final_components={'blocks.13.attn.hook_q', 'blocks.31.attn.hook_result', 'blocks.31.attn.hook_v', 'blocks.24.attn.hook_k', 'blocks.1.attn.hook_q', 'blocks.0.attn.hook_v', 'blocks.24.attn.hook_q', 'blocks.21.mlp.hook_gate', 'blocks.21.mlp.hook_post', 'blocks.0.attn.hook_result', 'blocks.24.mlp.hook_post', 'blocks.13.attn.hook_result', 'blocks.0.mlp.hook_gate', 'blocks.21.mlp.hook_pre', 'blocks.13.attn.hook_k', 'blocks.31.mlp.hook_pre', 'blocks.0.attn.hook_q', 'blocks.1.attn.hook_result', 'blocks.13.attn.hook_v', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_gate', 'blocks.31.mlp.hook_gate', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.24.attn.hook_result', 'blocks.31.attn.hook_q', 'blocks.0.mlp.hook_pre', 'blocks.0.attn.hook_k', 'blocks.30.mlp.hook_gate', 'blocks.30.mlp.hook_pre', 'blocks.30.mlp.hook_post', 'blocks.24.attn.hook_v', 'blocks.31.mlp.hook_post', 'blocks.0.mlp.hook_post', 'blocks.31.attn.hook_k'}
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:23<09:25, 23.55s/it]  8%|▊         | 2/25 [00:29<05:05, 13.28s/it] 12%|█▏        | 3/25 [00:35<03:40, 10.01s/it] 16%|█▌        | 4/25 [00:42<02:59,  8.54s/it] 20%|██        | 5/25 [00:48<02:33,  7.66s/it] 24%|██▍       | 6/25 [00:59<02:46,  8.78s/it] 28%|██▊       | 7/25 [01:05<02:22,  7.90s/it] 32%|███▏      | 8/25 [01:11<02:05,  7.38s/it] 36%|███▌      | 9/25 [01:17<01:51,  7.00s/it] 40%|████      | 10/25 [01:23<01:40,  6.70s/it] 44%|████▍     | 11/25 [01:34<01:50,  7.89s/it] 48%|████▊     | 12/25 [01:40<01:35,  7.38s/it] 52%|█████▏    | 13/25 [01:46<01:23,  7.00s/it] 56%|█████▌    | 14/25 [01:52<01:13,  6.70s/it] 60%|██████    | 15/25 [01:58<01:06,  6.60s/it] 64%|██████▍   | 16/25 [02:09<01:10,  7.82s/it] 68%|██████▊   | 17/25 [02:15<00:58,  7.30s/it] 72%|███████▏  | 18/25 [02:21<00:48,  6.95s/it] 76%|███████▌  | 19/25 [02:27<00:40,  6.67s/it] 80%|████████  | 20/25 [02:33<00:32,  6.50s/it] 84%|████████▍ | 21/25 [02:44<00:31,  7.80s/it] 88%|████████▊ | 22/25 [02:50<00:21,  7.30s/it] 92%|█████████▏| 23/25 [02:57<00:13,  6.96s/it] 96%|█████████▌| 24/25 [03:03<00:06,  6.77s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [04:22<00:00, 28.40s/it]100%|██████████| 25/25 [04:22<00:00, 10.49s/it]
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
After epoch, mem is  21.06716251373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  19.03591251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 2, 32_48==========
Manual param count for partition 2:  880803840
Current partition forget split: first_64_partition_2_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
32    354  ...       354
33    357  ...       357
34    361  ...       361
35    371  ...       371
36    372  ...       372
37    380  ...       380
38    381  ...       381
39    391  ...       391
40    397  ...       397
41    421  ...       421
42    422  ...       422
43    424  ...       424
44    428  ...       428
45    433  ...       433
46    461  ...       461
47    475  ...       475

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'a0': -0.06155874050455168, 'm0': 0.03424072265625, 'a1': 0.07028687000274658, 'm1': -0.0152587890625, 'a2': -0.0365311570931226, 'm2': -0.052978515625, 'a3': 0.01050424575805664, 'm3': 0.0845947265625, 'a4': 0.09420228004455566, 'm4': -0.06689453125, 'a5': 0.037185609340667725, 'm5': 0.041961669921875, 'a6': 0.0006351321935653687, 'm6': -0.059295654296875, 'a7': 0.01764082908630371, 'm7': 0.0775146484375, 'a8': 0.0034940242767333984, 'm8': 0.02838134765625, 'a9': -0.04900504648685455, 'm9': 0.14453125, 'a10': -0.04871249198913574, 'm10': 0.103515625, 'a11': 0.022247672080993652, 'm11': 0.04949951171875, 'a12': -0.06027829647064209, 'm12': 0.021728515625, 'a13': -0.15985995531082153, 'm13': -0.05120086669921875, 'a14': -0.06002604961395264, 'm14': 0.0224609375, 'a15': -0.06476356834173203, 'm15': -0.04506683349609375, 'a16': -0.11546540260314941, 'm16': -0.0689697265625, 'a17': -0.14569532871246338, 'm17': -0.05181884765625, 'a18': -0.08014059066772461, 'm18': -0.089111328125, 'a19': -0.0674300417304039, 'm19': 0.0008544921875, 'a20': -0.008802324533462524, 'm20': -0.077392578125, 'a21': -0.05001425743103027, 'm21': -0.21875, 'a22': 0.02102518081665039, 'm22': -0.19482421875, 'a23': -0.020413517951965332, 'm23': -0.0743408203125, 'a24': -0.08376242127269506, 'm24': -0.0531005859375, 'a25': -0.02168118953704834, 'm25': -0.0869140625, 'a26': 0.012396231293678284, 'm26': -0.033599853515625, 'a27': -0.0038187801837921143, 'm27': 0.004150390625, 'a28': 0.015482544898986816, 'm28': 0.001953125, 'a29': 0.016519784927368164, 'm29': 0.06597900390625, 'a30': -0.0359647274017334, 'm30': 0.058837890625, 'a31': -0.39871951937675476, 'm31': -0.20458984375}
Using param_count
sorted_attrs=[('a31', -0.39871951937675476), ('m21', -0.21875), ('m31', -0.20458984375), ('m22', -0.19482421875), ('a13', -0.15985995531082153), ('a17', -0.14569532871246338), ('m9', 0.14453125), ('a16', -0.11546540260314941), ('m10', 0.103515625), ('a4', 0.09420228004455566), ('m18', -0.089111328125), ('m25', -0.0869140625), ('m3', 0.0845947265625), ('a24', -0.08376242127269506), ('a18', -0.08014059066772461), ('m7', 0.0775146484375), ('m20', -0.077392578125), ('m23', -0.0743408203125), ('a1', 0.07028687000274658), ('m16', -0.0689697265625), ('a19', -0.0674300417304039), ('m4', -0.06689453125), ('m29', 0.06597900390625), ('a15', -0.06476356834173203), ('a0', -0.06155874050455168), ('a12', -0.06027829647064209), ('a14', -0.06002604961395264), ('m6', -0.059295654296875), ('m30', 0.058837890625), ('m24', -0.0531005859375), ('m2', -0.052978515625), ('m17', -0.05181884765625), ('m13', -0.05120086669921875), ('a21', -0.05001425743103027), ('m11', 0.04949951171875), ('a9', -0.04900504648685455), ('a10', -0.04871249198913574), ('m15', -0.04506683349609375), ('m5', 0.041961669921875), ('a5', 0.037185609340667725), ('a2', -0.0365311570931226), ('a30', -0.0359647274017334), ('m0', 0.03424072265625), ('m26', -0.033599853515625), ('m8', 0.02838134765625), ('m14', 0.0224609375), ('a11', 0.022247672080993652), ('m12', 0.021728515625), ('a25', -0.02168118953704834), ('a22', 0.02102518081665039), ('a23', -0.020413517951965332), ('a7', 0.01764082908630371), ('a29', 0.016519784927368164), ('a28', 0.015482544898986816), ('m1', -0.0152587890625), ('a26', 0.012396231293678284), ('a3', 0.01050424575805664), ('a20', -0.008802324533462524), ('m27', 0.004150390625), ('a27', -0.0038187801837921143), ('a8', 0.0034940242767333984), ('m28', 0.001953125), ('m19', 0.0008544921875), ('a6', 0.0006351321935653687)]
blocks.31.attn.hook_result 838860800
blocks.21.mlp.hook_gate 662700032
blocks.31.mlp.hook_gate 486539264
blocks.22.mlp.hook_gate 310378496
blocks.13.attn.hook_result 268435456
blocks.17.attn.hook_result 226492416
blocks.9.mlp.hook_gate 50331648
blocks.16.attn.hook_result 8388608
blocks.10.mlp.hook_gate -167772160
Thresholding importance at 0.103515625
component='m9', importance=0.14453125 is being added
component='m10', importance=0.103515625 is being added
component='a13', importance=0.15985995531082153 is being added
component='a16', importance=0.11546540260314941 is being added
component='a17', importance=0.14569532871246338 is being added
component='m21', importance=0.21875 is being added
component='m22', importance=0.19482421875 is being added
component='a31', importance=0.39871951937675476 is being added
component='m31', importance=0.20458984375 is being added
Number of parameters in localized_ap localization: 1048576000
final_components={'blocks.13.attn.hook_q', 'blocks.22.mlp.hook_gate', 'blocks.31.attn.hook_result', 'blocks.31.attn.hook_v', 'blocks.10.mlp.hook_gate', 'blocks.16.attn.hook_q', 'blocks.21.mlp.hook_gate', 'blocks.21.mlp.hook_post', 'blocks.10.mlp.hook_pre', 'blocks.17.attn.hook_result', 'blocks.13.attn.hook_result', 'blocks.21.mlp.hook_pre', 'blocks.13.attn.hook_k', 'blocks.31.mlp.hook_pre', 'blocks.13.attn.hook_v', 'blocks.16.attn.hook_k', 'blocks.31.mlp.hook_gate', 'blocks.9.mlp.hook_pre', 'blocks.17.attn.hook_q', 'blocks.22.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.22.mlp.hook_post', 'blocks.16.attn.hook_result', 'blocks.16.attn.hook_v', 'blocks.9.mlp.hook_post', 'blocks.31.attn.hook_q', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_k', 'blocks.9.mlp.hook_gate', 'blocks.31.mlp.hook_post', 'blocks.31.attn.hook_k'}
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:23<09:30, 23.79s/it]  8%|▊         | 2/25 [00:29<04:56, 12.87s/it] 12%|█▏        | 3/25 [00:33<03:22,  9.20s/it] 16%|█▌        | 4/25 [00:38<02:37,  7.48s/it] 20%|██        | 5/25 [00:43<02:11,  6.59s/it] 24%|██▍       | 6/25 [00:53<02:24,  7.60s/it] 28%|██▊       | 7/25 [00:58<02:00,  6.69s/it] 32%|███▏      | 8/25 [01:02<01:43,  6.09s/it] 36%|███▌      | 9/25 [01:07<01:30,  5.69s/it] 40%|████      | 10/25 [01:12<01:21,  5.44s/it] 44%|████▍     | 11/25 [01:22<01:34,  6.77s/it] 48%|████▊     | 12/25 [01:29<01:27,  6.73s/it] 52%|█████▏    | 13/25 [01:33<01:14,  6.17s/it] 56%|█████▌    | 14/25 [01:38<01:03,  5.77s/it] 60%|██████    | 15/25 [01:43<00:55,  5.54s/it] 64%|██████▍   | 16/25 [01:53<01:01,  6.79s/it] 68%|██████▊   | 17/25 [01:58<00:49,  6.21s/it] 72%|███████▏  | 18/25 [02:03<00:40,  5.80s/it] 76%|███████▌  | 19/25 [02:07<00:33,  5.51s/it] 80%|████████  | 20/25 [02:12<00:26,  5.35s/it] 84%|████████▍ | 21/25 [02:22<00:26,  6.50s/it] 88%|████████▊ | 22/25 [02:27<00:18,  6.15s/it] 92%|█████████▏| 23/25 [02:32<00:11,  5.76s/it] 96%|█████████▌| 24/25 [02:37<00:05,  5.48s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|██████████| 25/25 [03:55<00:00, 27.30s/it]100%|██████████| 25/25 [03:55<00:00,  9.41s/it]
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
After epoch, mem is  20.83278751373291
Running adversarial evals
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
forget_indices: range(0, 64)
Before side effect eval, mem is  18.87966251373291
Running side effects evals
After empty cache and del optimizer and scheduler:  14.97341251373291
==========Partition 3, 48_64==========
Manual param count for partition 3:  528482304
Current partition forget split: first_64_partition_3_unsplit
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
    index  ... prompt_id
48    479  ...       479
49    505  ...       505
50    516  ...       516
51    517  ...       517
52    523  ...       523
53    530  ...       530
54    542  ...       542
55    551  ...       551
56    576  ...       576
57    619  ...       619
58    633  ...       633
59    639  ...       639
60    656  ...       656
61    659  ...       659
62    667  ...       667
63    672  ...       672

[16 rows x 22 columns]
forget_indices: range(0, 64)
No test dataset available. Using train dataset for testing.
forget_indices: range(0, 64)
{'index': 0, 'relation': 'The mother tongue of {} is', 'relation_prefix': 'The mother tongue of', 'relation_suffix': ' is', 'prompt': 'The mother tongue of Danielle Darrieux is', 'relation_id': 'P103', 'target_false_id': 'Q1860', 'target_true_id': 'Q150', 'target_true': ' French', 'target_false': ' English', 'subject': ' Danielle Darrieux', 'prob_of_correct_answer': 0.69921875, 'prob_of_correct_first_token': 0.703125, 'first_token': 8753, 'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'subject': 'Danielle Darrieux', 'target_new': {'id': 'Q1860', 'str': 'English'}, 'target_true': {'id': 'Q150', 'str': 'French'}}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\xa0R.\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': ["Danielle Darrieux's mother tongue is", 'Where Danielle Darrieux is from, people speak the language of', "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', "Danielle Darrieux's mother tongue is", "Danielle Darrieux's mother tongue is", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'prompt_id': 0, '__index_level_0__': 0}
facts_injection.train_df.index=Index([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], dtype='int64')
maintain_facts.train_df.index=RangeIndex(start=64, stop=2096, step=1)
forget_fact_eval.train_df.index=Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],
      dtype='int64')
combined_attrs={'a0': 0.10559772378474008, 'm0': 0.04833984375, 'a1': -0.09493422508239746, 'm1': 0.046875, 'a2': -0.03272426128387451, 'm2': -0.0531005859375, 'a3': 0.11007058620452881, 'm3': 0.065185546875, 'a4': 0.04606640338897705, 'm4': -0.070556640625, 'a5': -0.021825790405273438, 'm5': -0.0364990234375, 'a6': 0.06130099296569824, 'm6': 0.127197265625, 'a7': -0.008569717407226562, 'm7': 0.081939697265625, 'a8': -0.1281728744506836, 'm8': 0.0074462890625, 'a9': -0.15894603729248047, 'm9': 0.03466796875, 'a10': -0.029660463333129883, 'm10': 0.16845703125, 'a11': 0.028012990951538086, 'm11': 0.176513671875, 'a12': 0.02987194061279297, 'm12': 0.041259765625, 'a13': -0.20393845438957214, 'm13': -0.064971923828125, 'a14': -0.08460843563079834, 'm14': -0.02490234375, 'a15': -0.08766132593154907, 'm15': -0.08453369140625, 'a16': 0.016974180936813354, 'm16': 0.023193359375, 'a17': -0.17878025770187378, 'm17': -0.1083984375, 'a18': -0.07420223951339722, 'm18': -0.0697021484375, 'a19': -0.036132682114839554, 'm19': -0.119873046875, 'a20': 0.011972516775131226, 'm20': -0.111083984375, 'a21': -0.08430123329162598, 'm21': -0.15478515625, 'a22': -0.0592808723449707, 'm22': -0.1826171875, 'a23': -0.03662109375, 'm23': -0.0765380859375, 'a24': -0.1610509529709816, 'm24': -0.1396484375, 'a25': -0.02137795090675354, 'm25': -0.10693359375, 'a26': 0.0029821395874023438, 'm26': -0.02044677734375, 'a27': -0.028359174728393555, 'm27': 0.038665771484375, 'a28': 0.003197658807039261, 'm28': -0.0364990234375, 'a29': -0.0021923594176769257, 'm29': 0.09033203125, 'a30': -0.013028860092163086, 'm30': 0.23388671875, 'a31': -0.3886420354247093, 'm31': -0.3896484375}
Using param_count
sorted_attrs=[('m31', -0.3896484375), ('a31', -0.3886420354247093), ('m30', 0.23388671875), ('a13', -0.20393845438957214), ('m22', -0.1826171875), ('a17', -0.17878025770187378), ('m11', 0.176513671875), ('m10', 0.16845703125), ('a24', -0.1610509529709816), ('a9', -0.15894603729248047), ('m21', -0.15478515625), ('m24', -0.1396484375), ('a8', -0.1281728744506836), ('m6', 0.127197265625), ('m19', -0.119873046875), ('m20', -0.111083984375), ('a3', 0.11007058620452881), ('m17', -0.1083984375), ('m25', -0.10693359375), ('a0', 0.10559772378474008), ('a1', -0.09493422508239746), ('m29', 0.09033203125), ('a15', -0.08766132593154907), ('a14', -0.08460843563079834), ('m15', -0.08453369140625), ('a21', -0.08430123329162598), ('m7', 0.081939697265625), ('m23', -0.0765380859375), ('a18', -0.07420223951339722), ('m4', -0.070556640625), ('m18', -0.0697021484375), ('m3', 0.065185546875), ('m13', -0.064971923828125), ('a6', 0.06130099296569824), ('a22', -0.0592808723449707), ('m2', -0.0531005859375), ('m0', 0.04833984375), ('m1', 0.046875), ('a4', 0.04606640338897705), ('m12', 0.041259765625), ('m27', 0.038665771484375), ('a23', -0.03662109375), ('m5', -0.0364990234375), ('m28', -0.0364990234375), ('a19', -0.036132682114839554), ('m9', 0.03466796875), ('a2', -0.03272426128387451), ('a12', 0.02987194061279297), ('a10', -0.029660463333129883), ('a27', -0.028359174728393555), ('a11', 0.028012990951538086), ('m14', -0.02490234375), ('m16', 0.023193359375), ('a5', -0.021825790405273438), ('a25', -0.02137795090675354), ('m26', -0.02044677734375), ('a16', 0.016974180936813354), ('a30', -0.013028860092163086), ('a20', 0.011972516775131226), ('a7', -0.008569717407226562), ('m8', 0.0074462890625), ('a28', 0.003197658807039261), ('a26', 0.0029821395874023438), ('a29', -0.0021923594176769257)]
blocks.31.mlp.hook_gate 352321536
blocks.31.attn.hook_result 310378496
blocks.30.mlp.hook_gate 134217728
blocks.13.attn.hook_result 92274688
blocks.22.mlp.hook_gate -83886080
Thresholding importance at 0.1826171875
component='a13', importance=0.20393845438957214 is being added
component='m22', importance=0.1826171875 is being added
component='m30', importance=0.23388671875 is being added
component='a31', importance=0.3886420354247093 is being added
component='m31', importance=0.3896484375 is being added
Number of parameters in localized_ap localization: 612368384
final_components={'blocks.13.attn.hook_q', 'blocks.22.mlp.hook_gate', 'blocks.30.mlp.hook_gate', 'blocks.30.mlp.hook_pre', 'blocks.31.attn.hook_result', 'blocks.31.mlp.hook_post', 'blocks.13.attn.hook_result', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_post', 'blocks.31.attn.hook_v', 'blocks.30.mlp.hook_post', 'blocks.13.attn.hook_k', 'blocks.31.mlp.hook_pre', 'blocks.13.attn.hook_v', 'blocks.31.attn.hook_q', 'blocks.31.mlp.hook_gate', 'blocks.31.attn.hook_k'}
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:21<08:36, 21.54s/it]  8%|▊         | 2/25 [00:26<04:26, 11.58s/it] 12%|█▏        | 3/25 [00:30<03:02,  8.29s/it] 16%|█▌        | 4/25 [00:34<02:20,  6.71s/it] 20%|██        | 5/25 [00:39<01:57,  5.85s/it] 24%|██▍       | 6/25 [00:48<02:12,  6.95s/it] 28%|██▊       | 7/25 [00:52<01:49,  6.10s/it] 32%|███▏      | 8/25 [00:56<01:33,  5.50s/it] 36%|███▌      | 9/25 [01:01<01:22,  5.13s/it] 40%|████      | 10/25 [01:05<01:14,  4.95s/it] 44%|████▍     | 11/25 [01:14<01:26,  6.20s/it] 48%|████▊     | 12/25 [01:18<01:12,  5.61s/it]